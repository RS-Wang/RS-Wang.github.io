---
layout:     post   				                            # 使用的布局（不需要改）
title:      聚类				                   # 标题 
subtitle:   K-mean/密度聚类/层次聚类            # 副标题
date:       2018-12-08 				                        # 时间
author:     RS-Wang 						                      # 作者
header-img: img/post-bg-ml2.jpg 	                    #这篇文章标题背景图片
catalog: true 						                            # 是否归档
usemathjax: true
tags:								                                  #标签
tags:                                                                 #标签
    - 机器学习
    - K-mean
    - 密度聚类
    - 层次聚类
    - 无监督学习
---

### 1.性能度量
> 聚类性能度量也称为聚类“有效指标”。用来评价聚类结果的好坏，和用于聚类过程中的优化目标
#### 1.1.外部指标
> 外部指标：是将结果与某个“参考模型”进行比较
>
> 对数据集$$D=\{x_1,x_2,...,x_m\}$$，令通过聚类给出的簇划分为$$C=\{C_1,C_2,...,C_k\}$$,参考模型给出的簇划分为$$C^*=\{C^*_1,C^*_2,...,C^*_s\}$$.令$$y$$与$$y^*$$分别为$$C$$和$$C^*$$对应的簇标记向量
>
> $$a=\mid SS\mid,SS=\{(x_i,x_j)\mid y_i=y_j,y^*_i=y^*_j,i<j\}$$,
> 
> $$b=\mid SD\mid,SD=\{(x_i,x_j)\mid y_i=y_j,y^*_i\neq y^*_j,i<j\}$$,
> 
> $$c=\mid DS\mid,DS=\{(x_i,x_j)\mid y_i\neq y_j,y^*_i=y^*_j,i<j\}$$,
> 
> $$d=\mid DD\mid,DD=\{(x_i,x_j)\mid y_i\neq y_j,y^*_i\neq y^*_j,i<j\}$$,
> 
> - 集合SS包含了在$$C$$中隶属于相同的簇且在$$C^*$$中也隶属于相同簇的样本对。
> - 集合SD包含了在$$C$$中隶属于相同的簇且在$$C^*$$中不隶属于相同簇的样本对。
> - 每个样本对$$(x_i,x_j)(i<j)$$仅能出现在一个集合中，因此有$$a+b+c+d=m(m-1)/2$$

- Jaccard系数：$$JC=\frac{a}{a+b+c}$$

- FM指数：$$FMI=\sqrt{\frac{a}{a+b}\cdot\frac{a}{a+c}}$$

- Rand指数：$$RI=\frac{2(a+d)}{m(m-1)}$$

> 上述性能度量的结果均在[0,1]区间，值越大越好。

#### 1.2.内部指标
> 令通过聚类给出的簇划分为$$C=\{C_1,C_2,...,C_k\}$$
> 
> $$avg(C)=\frac{2}{\mid C\mid(\mid C\mid-1)}\sum_{1\leq i\leq j\leq\mid C\mid}dist(x_i,x_j)$$,
> 
> $$diam(C)=\max_{1\leq i\leq j\leq\mid C\mid}dist(x_i,x_j)$$,
> 
> $$d_{min}(C_i,C_j)=\min_{x_i\in C_i,x_j\in C_j}dist(x_i,x_j)$$,
> 
> $$d_{cen}(C_i,C_j)=dist(\mu_i,\mu_j)$$,
> 
> - 其中$$dist(.,.)$$表示两个样本之间的距离。
> - $$\mu$$代表簇C的中心点$$\mu=\frac{1}{\mid C\mid}\sum_{1\leq i\leq\mid C\mid}x_i$$。
> - $$avg(C)$$代表簇C中样本间的平均距离。
> - $$diam(C)$$表示簇C内样本间最远距离。
> - $$d_{min}(C_i,C_j)$$对应于簇$$C_i$$与$$C_j$$最近样本间的距离。
> - $$d_{cen}(C_i,C_j)$$对应于簇$$C_i$$与$$C_j$$中心点间的距离

- DB指数：$$DBI=\frac{1}{k}\sum^k_{i=1}\max_{i\neq j}(\frac{avg(C_i)+avg(C_j)}{d_{cen}(C_i,C_j)})$$
    - DBI的值越小越好 
- Dunn指数：$$DI=\min_{1\leq i\leq k}\{\min_{i\neq j}(\frac{d_{min}(C_i,C_j)}{\max_{1\leq l\leq k}dist(C_l)})\}$$
    - DI的值越大越好

### 2.距离计算
#### 2.1.有序属性
> 定义域为{1,2,3}的离散属性与连续属性的性质更接近，能直接在属性值上计算距离，1与2比较接近、与3比较远，这样的属性称为‘有序属性’。

- 闵可夫斯基距离：$$dist_{mk}(x_i,x_j)=(\sum^n_{u=1}\mid x_{iu}-x_{ju}\mid^p)^{\frac{1}{p}}$$
    - u代表属性 
- 当p=2时，欧式距离：$$dist_{ed}(x_i,x_j)=\Vert x_i-x_j\Vert_2=\sqrt{\sum^n_{u=1}\mid x_{iu}-x_{ju}\mid^2}$$
- 当p=2时，曼哈顿距离：$$dist_{man}(x_i,x_j)=\Vert x_i-x_j\Vert_1=\sum^n_{u=1}\mid x_{iu}-x_{ju}\mid$$
 
#### 2.2.无序属性
> 定义域为{飞机，火车，轮船}这样的离散属性则不能直接在属性值上计算距离，称为‘无序属性’

- VDM：$$VDM_p(a,b)=\sum^k_{i=1}\mid \frac{m_{u,a,i}}{m_{u,a}}-\frac{m_{u,b,i}}{m_{u,b}}\mid^p$$
    - $$m_{u,a}$$表示属性u上取值为a的样本数
    - $$m_{u,a,i}$$表示在第i个样本簇中在属性u上取值为a的样本数
    - k为样本簇数

#### 2.3.混合属性
> 将闵可夫斯基距离和VDM结合即可处理混合属性，令样本有$$n_c$$个有序属性、$$n-n_c$$个无序属性，并将有序属性排列在无序属性之前。

- $$MinkovDM_p(x_i,x_j)=(\sum^{n_c}_{u=1}\mid x_{iu}-x_{ju}\mid^p + \sum^n_{u=n_c+1}VDM_p(x_{iu},x_{ju}))^{\frac{1}{p}}$$
 
### 3.原型聚类
> 原型聚类也称“基于原型的聚类”，此类算法假设聚类结构能通过一组原型刻画

#### 3.1.K均值算法
- 最小化平方误差：$$E=\sum^k_{i=1}\sum_{x\in C_i}\Vert x-\mu_i\Vert^2_2$$.
    - 其中$$\mu_i=\frac{1}{\mid C_i\mid}\sum_{x\in C_i}x$$是簇$$C_i$$的均值向量。
- 步骤：
    - 初始化：从数据集D中随机选择k个样本作为初始均值向量
    - 迭代直至收敛
        - 分配步骤：将每个数据点分配至距离最近的中心
        - 重拟合步骤：根据新的分配重新计算类聚中心
    - 最优聚类中心：当前cluster中所有数据的质心（算数平均值）
- 算法缺点：
    - 聚类中心$$\mu$$不一定属于数据集
    - 由于使用了L2距离函数，容易被离群点和噪声影响
- 时间复杂度：$$O(n)$$

### 4.密度聚类
> 密度聚类也称为“基于密度的聚类”，此类算法假设聚类结构能通过样本分布的紧密程度确定。密度聚类算法从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇以获得最终的聚类结果。

#### 4.1.DBSCAN 
> DBSCAN是一种密度聚类算法，他基于一组“领域”参数$$(\epsilon,MinPts)$$来刻画样本分布的紧密程度。样本集$$D={x_1,x_2,...,x_m}$$
> 
> - $$\epsilon$$-领域：对$$x_j\in D$$,它的领域是D中与$$x_j$$的距离不大于$$\epsilon$$的样本
> - 核心对象：若$$x_j$$的$$\epsilon$$-领域中的样本数大于等于MinPts,则$$x_j$$是一个核心对象。
> - 密度直达：若$$x_j$$在$$x_i$$的$$\epsilon$$-领域中，且$$x_i$$是核心对象，则$$x_j$$是$$x_i$$密度直达。
> - 密度可达：对$$x_i$$与$$x_j$$,若存在样本序列$$p_1,p_2,...,p_n$$,其中$$p_1=x_i,p_n=x_j$$,且$$p_{i+1}$$是$$p_i$$密度直达，则称$$x_j$$是$$x_i$$的密度可达。
> - 密度相连：对$$x_i$$与$$x_j$$,若存在$$x_k$$使得$$x_i$$与$$x_j$$均是$$x_k$$密度可达，则称$$x_i$$与$$x_j$$密度相连。
> 
> ![](https://note.youdao.com/yws/api/personal/file/D48875CD18884D13B8FFCC477BC5047A?method=download&shareKey=089f661df4ceda3b8d1982ba8c218cf3)
> 
> - MinPts=3,虚线显示出$$\epsilon$$-领域
>     - $$x_1$$是核心对象
>     - $$x_2$$由$$x_1$$密度直达
>     - $$x_3$$由$$x_1$$密度可达 
>     - $$x_3$$与$$x_4$$密度相连

- DBSCAN簇定义：由密度可达关系导出的最大的密度相连样本集合
- 步骤：
    - 找出所有的核心对象，形成集合$$\Omega$$ 
    - 循环到$$\Omega=\varnothing$$为止：
        - 从$$\Omega$$ 取出一个核心对象$$x_i$$
        - 找出由$$x_i$$密度可达的所有样本，形成集合$$C_k$$
        - $$\Omega=\Omega/C_k$$,将$$C_k$$中包含的核心对象从$$\Omega$$中剔除
    - 输出簇划分

### 5.层次聚类
> 层次聚类视图在不同层次对数据集进行划分，从而形成树形的聚类结构，数据集的划分可采用“自低向上”的聚合策略，也可采用“自顶向下”的拆分策略
#### 5.1.AGNES
> AGNES是一种采用自低向上的聚合策略的层次聚类，它先将数据集中每个样本看作一个初始聚类簇，然后在算法运行的每一步中找出距离最近的两个聚类进行合并，该过程不断重复，直至达到预设的聚类簇个数。
> 
> - 聚类簇$$C_i$$与$$C_j$$距离计算：
>     - 最小距离：$$d_{min}(C_i,C_j)= \min_{x\in C_i,z\in C_j}dist(x,z)$$,
>     - 最大距离：$$d_{min}(C_i,C_j)= \max_{x\in C_i,z\in C_j}dist(x,z)$$,
>     - 平均距离：$$d_{avg}(C_i,C_j)= \frac{1}{\mid C_i\mid\mid C_j\mid}\sum_{x\in C_i}\sum_{z\in C_j}dist(x,z)$$
> - 解释：
>     - 最小距离是由两个簇的最近样本决定，这时AGNES算法被称为“单链接”算法
>     - 最大距离是由两个簇最远样本决定，这时AGNES算法被称为“全链接”算法
>     - 平均距离有两个簇所有样本共同决定，这时AGNES算法被称为“均链接”算法     

- 步骤：
    - 设置聚类簇数k,当前簇数q(样本数)
    - 计算每个样本与其他样本的距离
    - 循环 
        - 找出距离最近的两个聚类簇并合并
        - 找出上面合并后的簇的最近的其他簇合并
        - 重复上面的步骤，直到当前的簇数q=k为止
    - 输出簇划分 

### 参考
- 《机器学习》周志华
