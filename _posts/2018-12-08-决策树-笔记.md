---
layout:     post   				                            # 使用的布局（不需要改）
title:      决策树				                   # 标题 
subtitle:   ID3/C4.5/CART/剪枝/连续值与缺失值            # 副标题
date:       2018-12-08 				                        # 时间
author:     RS-Wang 						                      # 作者
header-img: img/post-bg-ml2.jpg 	                    #这篇文章标题背景图片
catalog: true 						                            # 是否归档
usemathjax: true
tags:								                                  #标签
tags:                                                                 #标签
    - 机器学习
    - 决策树
    - ID3
    - C4.5
    - CART
    - 监督学习
---

## 决策树
### 1.决策树停止条件
- 1.1.条件一：
    - 当前结点包含的样本全属于同一个类别，无需划分
- 1.2.条件二：
    - 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分
    - 这种情形下，把当前结点标记为叶子结点，并将其类别设定为该结点所含样本最多的类别
    - 相当于后验分布
- 1.3.条件三：
    - 当前结点包含的样本集合为空，不能划分
    - 这种情形下，把当前结点标记为叶子结点，并将其类别设定为其父节点所含样本最多的类别
    - 相当于先验分布

### 2.划分选择
#### 2.1.ID3
- 1.信息熵：
    - 含义：是度量样本集合纯度的一个指标
    - 公式：$$Ent(D)=-\sum^{\mid y\mid}_{k=1}P_k\log_2 P_k$$
        - $$P_k$$是第k类的样本所占的比例 
    - 意义：Ent(D)的值越小，则D的纯度越高
- 2.信息增益：
    - 含义：使用属性a来划分所获得的‘纯度提升’
    - 公式：$$Gain(D,a)=Ent(D)-\sum^V_{v=1}\frac{\mid D^v\mid}{\mid D^v\mid}Ent(D^v)$$
        - a是样本的一个离散属性，有V个可能的取值$${a^1,a^2,...,a^V}$$
    - 意义：信息增益越大，说明根据这个属性划分所获得的‘纯度提升’越大 
    - **信息增益准则对取值数目较多的的属性有所偏好** 
- 3.ID3:
    - 划分方式:
        - 就是以信息增益最大的属性为划分属性
    
#### 2.2.C4.5
- 1.信息增益率：
    - 公式：$$Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$$ 
        - 其中$$IV(a)=-\sum^V_{v=1}\frac{\mid D^v\mid}{\mid D\mid}log_2\frac{\mid D^v\mid}{\mid D\mid}$$ 
    - **信息增益率准则对可取数值较少的属性有所偏好**
- 2.C4.5：
    - 划分方式（启发式）：
        - 先从候选划分属性中找出信息增益高于平均水平的属性
        - 再从中选择增益率最高的
    - 解决了ID3中对取值数目较多的的属性有所偏好的问题

#### 2.3.CART
- 1.基尼值：
    - 含义：反映了从数据集D中随机抽取两个样本，期类别不一致的概率 
    - 公式：$$Gini(D)=\sum^{\mid y\mid}_{k=1}\sum_{k'\neq k}P_kP_{k'}=1-\sum^{\mid y\mid}_{k=1}P_k^2$$
    - 意义：Gini(D)值越小，则数据集D的纯度越高
- 2.基尼指数：
    - 含义：属性a的基尼值
    - 公式：$$Gini_index(D,a)=\sum^V_{v=1}\frac{\mid D^v\mid}{\mid D\mid}Gini(D^v)$$
- 3.CART：
    - 划分方式:
        - 选择基尼指数最小的属性为最优划分属性
         
### 5.剪枝
#### 5.1.作用
- 剪枝是决策树学习算法防止过拟合的主要手段

#### 5.2.预剪枝
- 1.预剪枝：
    - 是指在决策树生成过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点
- 2.流程：
    - 1.将数据集分为训练集和验证集
    - 2.用一个决策树算法并用训练集训练，找出一个划分属性
    - 3.用这个划分属性去划分验证集，看划分后的精度是否提高
        - 3.1.提高就确定这个划分属性，并继续用训练集进行训练，然后在用验证集验证，降低就停止这个结点划分
        - 3.2.若没有提高则停止属性划分
- 3.优缺点：
    - 优点：
        - 预剪枝使得决策树的很多分支都没有‘展开’，降低了过拟合的风险
        - 同时显著减少了决策树训练时间开销和测试时间开销
    - 缺点：
        - 由于预剪枝禁止了很多的分支展开，有欠拟合的风险  

#### 5.3.后剪枝
- 1.后剪枝：
    - 是先从训练集生成一颗完整的树，然后自底向上地对非叶子结点进行考察，若将该结点对应的子树替换成叶子结点能带来决策树泛化性能提升，则将该子树替换为叶结点
- 2.流程：
    - 1.将数据集分为训练集和验证集
    - 2.使用训练集训练出一个完整的决策树
    - 3.使用验证集对这个完整的决策树进行自底向上的剪枝。将最底层的子树替换成叶子结点，看精度是否提高
        - 3.1.提高就剪枝
        - 3.2.若没有提高就停止这个结点的剪枝换别的子树继续验证
- 3.优缺点：
    - 优点：
        - 后剪枝决策树通常比预剪枝要保留更多的分支，但过拟合的风险很小
        - 后剪枝决策树的泛化性能往往要优于预剪枝决策树
    - 缺点：
        - 后剪枝决策树的训练时间开销比预剪枝决策树要大得多 

### 6.连续值与缺失值
#### 6.1.连续值
- 1.基本思路：
    - 连续属性离散化：使用二分法连续属性进行处理
- 2.二分法处理流程：
    - 1.将样本连续属性a中的值，提取出不同的取值，然后从小到大排序
    - 2.对连续属性a,可考察包含n-1个元素的候选划分点集合$$T_a=\{\frac{a^i+a^{i+1}}{2}\mid 1\leq i \leq n-1 \}$$,即对排序后的相邻两个连续值求均值  
- 3.连续值属性划分：
    - 将$$T_a$$中的每个值都带入决策树算法中，找到合适的划分属性值t
 
#### 6.2.缺失值
- 1.基本思路：
    - 样本赋权，权重划分
- 2.属性值缺失的情况下进行划分属性选择
    - 1.令$$\hat{D}$$表示D中在属性a中没有缺失值的样本子集，并为每个样本x赋予一个权重$W_x$
    - 2.定义
        - $$\rho=\frac{\sum_{x\in\hat{D}}W_x}{\sum_{x\in D}W_x}$$表示无缺失值样本所占的比例
        - $$\hat{P_k}=\frac{\sum_{x\in\hat{D_k}}W_x}{\sum_{x\in\hat{D}}W_x} \ (1\leq k\leq\mid y\mid)$$表示无缺失值样本第k类所占的比例
        - $$\hat{r_v}=\frac{\sum_{x\in\hat{D^v}}W_x}{\sum_{x\in\hat{D}}W_x} \ (1\leq v\leq V)$$表示无缺失值样本中在属性a上取值$a^v$的样本所占的比例
    - 3.计算信息增益
        - 信息熵：$$Ent(\hat{D})=-\sum^{\mid y\mid}_{k=1}\hat{P_k}\log_2\hat{P_k}$$
        - 信息增益：$$Gain(D,a)=\rho\cdot Gain(\hat{D},a)=\rho\cdot(Ent(\hat{D})-\sum^V_{v=1}\hat{r_v}Ent(\hat{D^v}))$$
- 3.确定划分属性，样本在该属性上的值缺失下进行划分
    - 1.若样本x在划分属性a上的取值已知，则将x划入与其对应的子节点中，且样本权重保持为$W_x$
    - 2.若样本x在划分属性a上的取值为知，则将x划入所有子结点，且样本权重在与属性值$a^v$对应的子结点中调整为$$\hat{r_v}\cdot W_x$$

### 参考
- 《机器学习》周志华
