---
layout:     post   				                            # 使用的布局（不需要改）
title:      集成学习				                   # 标题 
subtitle:   Boosting/Bagging/Stacking            # 副标题
date:       2018-12-08 				                        # 时间
author:     RS-Wang 						                      # 作者
header-img: img/post-bg-ml2.jpg 	                    #这篇文章标题背景图片
catalog: true 						                            # 是否归档
usemathjax: true
tags:								                                  #标签
tags:                                                                 #标签
    - 机器学习
    - Boosting
    - Bagging
    - Stacking
    - 监督学习
---

### 1.Boosting
#### 1.1.介绍
- Boosting是个体学习器间存在强依赖关系且必须串行生成序列化方法
- Boosting是一族可将弱学习器提升为强学习器的算法

#### 1.2.工作机制
- 先从初始训练集训练处一个基学习器
- 再根据结果对训练集的样本分布进行调整，使得先前基学习器做错的样本在后续受到更多关注
- 基于调整后的的训练样本分布训练下一个基学习器 
- 如此反复进行，直至基学习器数目达到事先指定的值T
- 最终将这T个学习器进行加权结合

#### 1.3.数据重分布
- 重赋权法：
    - 即在训练过程中的每一轮中，根据样本分布为每个训练样本重新赋予一个权重 
- 重采样法：
    - 即在训练过程中的每一轮中，根据样本分布对训练集重新进行采样，再用重采样得到的样本集对基学习器进行训练
- 对比：
    - 使用重赋权法，Boosting算法在检查当前生成的基学习算法是否满足条件，一旦条件不满足，则当前的基学习器就被抛弃，且学习过程会停止。这种情况下，可能没有达到学习轮数T，可能导致最终集成中只包含很少的基学习器而性能不佳。
    - 使用重采样法，Boosting算法在检查当前生成的基学习算法是否满足条件，一旦条件不满足，则当前的基学习器就被抛弃，但可以根据当前分布重新对训练样本进行采样，再基于新的采样结果重新训练出基学习器，从而使得学习过程可以持续到预设的T轮完成

### 2.Bagging
#### 2.1.介绍
- Bagging是个体学习器间不存在强依赖关系且可以并行化方法
- 想得到泛化性能好的集成，那么需要集成中的个体学习器尽可能的相互独立（好而不同），Bagging的方法是通过自助采样的方法产出不同的训练子集来训练基学习器（一个子集对应一个基学习器），从而获得的基学习器具有较大的差异

#### 2.2.工作机制
- 从包含N个样本的原始样本集中抽取训练集。每轮从原始样本集中使用自助采样的方法抽取N个训练样本。进行M轮后得到M个训练集
- 每次使用一个训练集得到一个模型，M个训练集共得到M个模型
- 预测结果
    - 分类问题：将M个模型的结果使用投票法取得结果
    - 回归问题：将M个模型的结果使用平均法取得结果

### 3.Boosting与Bagging对比
- 样本选择上：
    - Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。
    - Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。
- 样例权重：
    - Bagging：使用均匀取样，每个样例的权重相等。
    - Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。
- 预测函数：
    - Bagging：所有预测函数的权重相等。
    - Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。
- 并行计算：
    - Bagging：各个预测函数可以并行生成。
    - Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。
- Boosting降低偏差，Bagging降低方差
    -  Boosting的偏差和方差
        - Boosting从优化角度来看，在迭代N步时，最小化N-1步的误差，这样一步步地最小化损失函数，其bias自然逐步下降。
        - 因为Boosting集成的子模型是顺序执行且强依赖的(强相关)，因此子模型之和并不能显著降低variance。

    -  Bagging的偏差和方差
        > 因为Bagging对样本重采样，对每一重采样得到的样本训练一个模型，最后取平均。由于子样本集的相似性以及使用的是同种模型，因此各模型有近似相等的bias和variance。
        - 由于$$E[\frac{\sum X_i}{n}]=E[X_i]$$,所以Bagging后的bias和单个子模型的接近，一般来说不能显著降低bias。
        - 若各子模型独立，则有$$Var(\frac{\sum X_i}{n})=\frac{Var(X_i)}{n}$$,此时可以显著降低variance。(若各子模型完全相同，则有$$Var(\frac{\sum X_i}{n})=Var(X_i)$$,此时不会降低variance。)

### 4.RF
#### 3.1.介绍
- 随机森林是Bagging的一个扩展变体
- RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择

#### 4.2.工作机制
- 从包含N个样本的原始样本集中抽取训练集。每轮从原始样本集中使用自助采样的方法抽取N个训练样本。进行M轮后得到M个训练集
- 每次使用一个训练集得到一个模型，在训练时只选取部分属性(推荐值k=$\log_2d$)，M个训练集共得到M个决策树
- 预测结果
    - 分类问题：将M个模型的结果使用投票法取得结果
    - 回归问题：将M个模型的结果使用平均法取得结果

#### 4.3.RF与Bagging性能对比
![](https://note.youdao.com/yws/api/personal/file/ED09A24AAC584E97AA61DBDC727BC262?method=download&shareKey=c407b8526f5071c519849f8602004859)

- RF简单、容易实现、计算开销小，训练效率比Bagging要高
- RF因为比Bagging多了属性扰动，使得个体学习器之间的差异度增加和泛化性能更好
- RF在起始性能比Bagging要差

### 5.Stacking
#### 5.1.介绍
- 当训练数据很多时，一种更为强大的结合策略是使用“学习法”，即通过另一个学习器来进行结合
- Stacking是学习法的典型代表
- Stacking先从初始数据集训练出初级学习器，然后“生成”一个新数据集用于训练次级学习器

#### 5.2.工作机制
- 将数据集分为训练集$$D_{Train}$$和测试集$$D_{Test}$$，使用训练集训练T个初级学习算法
- 训练后的初级学习器$$f_t$$，令$$z_it=f_t(x_i\in D_{Test})$$是一个初级学习器$$f_t$$对测试集中一个样本$$x_i$$的结果。那么所有初级学习器产生的结果为$$\mathbf{z}_i=(z_i1;z_i2;...;z_iT)$$,真实标记为$$\mathbf{y}_i$$
- 将所有的测试样本的预测结果和真实标记组成次级训练集$$D'=\{(\mathbf{z}_1,\mathbf{y}_1),(\mathbf{z}_2,\mathbf{y}_2),...,(\mathbf{z}_N,\mathbf{y}_N)\}$$
- 使用训练集$$D'$$训练次级学习器


### 参考
- 《机器学习》周志华
- [https://www.cnblogs.com/earendil/p/8872001.html](https://www.cnblogs.com/earendil/p/8872001.html)
- [https://www.zhihu.com/question/26760839](https://www.zhihu.com/question/26760839)
- [https://blog.csdn.net/shenxiaoming77/article/details/53894973](https://blog.csdn.net/shenxiaoming77/article/details/53894973)
