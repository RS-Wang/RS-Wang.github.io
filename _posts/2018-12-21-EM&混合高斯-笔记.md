---
layout:     post   				                            # 使用的布局（不需要改）
title:      EM&混合高斯				                   # 标题 
subtitle:   EM/混合高斯            # 副标题
date:       2018-12-21 				                        # 时间
author:     RS-Wang 						                      # 作者
header-img: img/post-bg-ml2.jpg 	                    #这篇文章标题背景图片
catalog: true 						                            # 是否归档
usemathjax: true
tags:								                                  #标签
tags:                                                                 #标签
    - 机器学习
    - EM
    - 混合高斯
    - 数学基础
    - 无监督学习
---

### 1.EM
#### 1.1.介绍
- EM算法是生成模型
- EM算法是最常见的隐变量(未观测变量)估计方法
- EM算法是一种迭代式的方法
- E步：推断出隐变量的期望
- M步：根据隐变量的期望做极大似然估计

#### 1.2.基础知识 
- 极大似然估计
    - 介绍
        - 极大似然估计是用的频率学派的思想
        - 认为任何数据都有与之应对的一种概率分布，即参数虽然未知，但客观存的固定值 
    - 求解步骤
        + 先假设这个函数分布是属于哪种概率分布
        + 写出似然函数：$$L(\theta)=\Pi^n_{i=1}P(x_i)$$
        + 写出对数似然：$$LL(\theta)=\sum^n_{i=1}\log P(x_i)$$
        + 求$$\frac{\partial LL(\theta)}{\partial\theta}=0$$
        + 求出的$$\theta$$就是该假设概率分布的参数
    
- 凸函数
    - [凸函数]({% post_url 2018-11-24-凸优化-笔记 %}) 
- Jensen不等式

    ![](https://note.youdao.com/yws/api/personal/file/65B80B8619034195BC683E863038099B?method=download&shareKey=b098e8db09cc65edb52e4530c896cc5d)

    - 如果函数f是凸函数，x是随机变量
    - 存在：$$E[f(x)]\geq f(E(x))$$
        - 如果函数f是严格凸函数，当且仅当随机变量是常量时等号成立
        - 若函数f是凹函数，Jensen不等式符号相反  

#### 1.3.EM算法推导
- 1.设定
    - 相互独立样本$$X=(\mathbf{x_1,x_2,...,x_n})$$为显变量
    - 与样本对应的未观测变量$$Z=(\mathbf{z_1,z_2,...,z_n})$$为隐变量
    - $$(X,Z)$$显变量和隐变量合起来才是是完全数据 
    - 样本的模型参数为$$\theta$$,则显变量的概率为$$P(\mathbf{x_i\mid\theta})$$
    - 完全数据概率$$P(\mathbf{x_i,z_i\mid\theta})$$
- 2.似然函数    
    - 对数似然：
        - 无隐变量：
          
          $$
          \begin{equation}
          \begin{aligned}
          LL(\theta)&=\log(\Pi^n_{i=1}P(\mathbf{x_i\mid\theta}))\\
          &=\sum^n_{i=1}log(P(\mathbf{x_i\mid\theta}))\\
          \end{aligned}
          \end{equation}
           $$

        - 有隐变量：
          
          $$
          \begin{equation}
          \begin{aligned}
          LL(\theta)&=\log(\Pi^n_{i=1}P(\mathbf{x_i,z_i\mid\theta})\\
          &=\sum^n_{i=1}\log\sum_{\mathbf{z}}(P(\mathbf{x_i,z_i\mid\theta}))\\
          \end{aligned}
          \end{equation}
          $$

    - 求取极大对数似然：
        - 无隐变量：
            - 直接求取$$\frac{\partial LL(\theta)}{\partial\theta}=0$$即可
        - 有隐变量：
            - 因为log中是加式，不容易求解

- 3.引入Jensen不等式
    - 对数似然变为：
      $$
      \begin{equation}
      \begin{aligned}
      LL(\theta)&=\log(\Pi^n_{i=1}P(\mathbf{x_i,z_i\mid\theta})\\
      &=\sum^n_{i=1}\log\sum_{\mathbf{z}}(P(\mathbf{x_i,z_i\mid\theta}))\\
      &=\sum^n_{i=1}\log\sum_{\mathbf{z}}Q(z_i)\frac{P(\mathbf{x_i,z_i\mid\theta})}{Q(z_i)}\\
      &\geq \sum^n_{i=1}\sum_{\mathbf{z}}Q(z_i)\log\frac{P(\mathbf{x_i,z_i\mid\theta})}{Q(z_i)}
      \end{aligned}
      \end{equation}
      $$

        - 其中加入$$Q(z_i)$$就是凑成函数的期望，即$$E(\frac{P(\mathbf{x_i,z_i\mid\theta})}{Q(z_i)})=\sum_{\mathbf{z}}Q(z_i)\frac{P(\mathbf{x_i,z_i\mid\theta})}{Q(z_i)}$$,
        - $$\sum_{\mathbf{z}}Q(z_i)=1,0\leq Q(z_i)\leq 1$$,
        - log是凹函数$$\log(E(Y))\leq E(\log(Y))$$,这时$$E(\log(Y))$$是$$\log(E(Y))$$下界
    
    - 让Jensen等式成立
        - 等式成立的条件是:$$\frac{P(\mathbf{x_i,z_i\mid\theta})}{Q(z_i)}=c$$，c是常数
        - 上式可变为:$$P(\mathbf{x_i,z_i\mid\theta})=cQ(z_i)$$  
        - 两边同累加：$$\sum_{\mathbf{z}}P(\mathbf{x_i,z_i\mid\theta})=c\sum_{\mathbf{z}}Q(z_i)=c$$
        - 联立上式可得：
          
          $$
          \begin{equation}
          \begin{aligned}
          Q(z_i)&=\frac{P(\mathbf{x_i,z_i\mid\theta})}{Q(z_i)}\\
          &=\frac{P(\mathbf{x_i,z_i\mid\theta})}{\sum_{\mathbf{z}}P(\mathbf{x_i,z_i\mid\theta})}\\
          &=\frac{P(\mathbf{x_i,z_i\mid\theta})}{P(\mathbf{x_i\mid\theta})}\\
          &=P(\mathbf{z_i\mid x_i,\theta})
          \end{aligned}
          \end{equation}
          $$
        
        > 从上式可以看出$$Q(z_i)$$就是隐变量分布

        - 极大化似然估计：
          
          $$
          \begin{equation}
          \begin{aligned}
          &\arg\max_{\theta}\sum^n_{i=1}\sum_{\mathbf{z}}Q(z_i)\log\frac{P(\mathbf{x_i,z_i\mid\theta})}{Q(z_i)}\\
          \Leftrightarrow &\arg\max_{\theta}\sum^n_{i=1}\sum_{\mathbf{z}}Q(z_i)\log P(\mathbf{x_i,z_i\mid\theta})
          \end{aligned}
          \end{equation}
          $$          

#### 1.4.EM算法流程
- 1.输入：观察数据$$X=(x_1,x_2,...,x_n)$$,迭代次数t
- 2.初始化模型参数$$\theta^0$$
- 3.for i in t
    - E步：根据当前$$\theta^i$$计算出隐变量分布$$P(\mathbf{z_i\mid x_i,\theta})$$,并计算对数似然$$LL(\theta)$$关于Z的期望，即
      
      $$
      \begin{equation}
      \begin{aligned}
      Q(\theta,\theta^{old})&=\sum^n_{i=1}\sum_{\mathbf{z}}Q(z_i)\log P(\mathbf{x_i,z_i\mid\theta})\\
      &=\sum^n_{i=1}\sum_{\mathbf{z}}P(\mathbf{z_i\mid x_i,\theta}^{old})\log P(\mathbf{x_i,z_i\mid\theta})
      \end{aligned}
      \end{equation}
      $$
    
    - M步：寻找参数最大化期望似然，即
      
      $$
      \begin{equation}
      \begin{aligned}
      \theta^{new}=\arg \max_{\theta}Q(\theta,\theta^{old})
      \end{aligned}
      \end{equation}
      $$

- 4.输出：模型参数$$\theta$$    

### 2.GMM混合高斯
#### 2.1.介绍
![](https://note.youdao.com/yws/api/personal/file/54C68DEBCC0345938E9FF88C05C6B9EC?method=download&shareKey=6aed02c3b4c410fbf7016ffd763aa670)

- 假定一个复杂的分布是由多个高斯分布组合而成 
- 这个复杂的分布就被称为混合高斯

#### 2.2.基础知识
- 高斯分布密度函数
    - 一元
        - 均值：$$\mu=\frac{1}{n}\sum^N_{n=1}x_n=\bar{x}$$
        - 方差：$$\sigma^2=\frac{1}{n}\sum^N_{n=1}(x_n-\bar{x})^2$$ 
        - 公式：$$\mathcal{N}(x\mid\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}\exp-\frac{(x-\mu)^2}{2\sigma^2}$$
    - 多元 
        - 均值：$$\mathbf{\mu}=\frac{1}{N}\sum^N_{n=1}\mathbf{x}_n$$
        - 方差：$$\Sigma=\frac{1}{n}\sum^N_{n=1}(\mathbf{x}_n-\mathbf{\mu})(\mathbf{x}_n-\mathbf{\mu})^T$$
        - 公式：$$\mathcal{N}(\mathbf{x\mid\mu,\Sigma})=\frac{1}{(2\pi)^{\frac{n}{2}}\mid\Sigma\mid^{\frac{1}{2}}}\exp-\frac{1}{2}(\mathbf{x}-\mathbf{\mu})^T\Sigma^{-1}(\mathbf{x}-\mathbf{\mu})$$

- 混合高斯分布
    - 公式：$$p_M(\mathbf{x})=\sum^K_{k=1}\pi_k\cdot\mathcal{N}(\mathbf{x}\mid\mathbf{\mu_k,\Sigma_k})$$  
        - 表示分布共由k个混合成分组成，每个混合成分对应一个高斯分布
        - $$\mathbf{\mu_k,\Sigma_k}$$是第k个高斯混合成分的参数
        - $$\pi_k>0$$为相应的“混合系数”,且$$\sum^K_{k=1}\pi_k=1$$   

#### 2.3.GMM求解(EM方法求解)
> 可以根据EM方式求解GMM，X是显变量，$$\mathbf{z}$$为K维隐变量向量，其中$$z_k=1$$其余剩余元素等于0，且有$$\sum^K_{k=1}z_k=1$$.同时有$$p(z_k=1)=\pi_k$$,即$$p(\mathbf{z})=\Pi^k_{k=1}\pi^{z_k}_k$$

- 条件分布
  
  $$
  \begin{equation}
  \begin{aligned}
  p(\mathbf{x}\mid z_k=1)&=\mathcal{N}(\mathbf{x}\mid\mathbf{\mu_k,\Sigma_k})\\
  \\
  p(\mathbf{x\mid z})&=\Pi^K_{k=1}\mathcal{N}(\mathbf{x}\mid\mathbf{\mu_k,\Sigma_k})^{z_k}
  \end{aligned}
  \end{equation}
  $$

- 边缘分布
  
  $$
  \begin{equation}
  \begin{aligned}
  p(\mathbf{x})&=\sum_{\mathbf{z}}p(\mathbf{x,z})\\
  &=\sum_{\mathbf{z}}p(\mathbf{z})p(\mathbf{x\mid z})\\
  &=\sum^K_{k=1}\pi_k\cdot\mathcal{N}(\mathbf{x}\mid\mathbf{\mu_k,\Sigma_k})
  \end{aligned}
  \end{equation}
  $$

- 根据贝叶斯，求出$$z_k$$的后验概率
  > 将$$\pi_k$$看成$$z_k=1$$的先验概率，将$$\gamma(z_k)$$看成观测到x之后，对应的后验概率
   
  $$
  \begin{equation}
  \begin{aligned}
  \gamma(z_k)&=p(z_k=1\mid\mathbf{x})\\
  &=\frac{p(z_k=1)p(\mathbf{x}\mid z_k=1)}{\sum^K_{j=1}p(z_j=1)p(\mathbf{x}\mid z_j=1)}\\
  &=\frac{\pi_k\cdot\mathcal{N}(\mathbf{x}\mid\mathbf{\mu_k,\Sigma_k})}{\sum^K_{j=1}\pi_j\cdot\mathcal{N}(\mathbf{x}\mid\mathbf{\mu_j,\Sigma_j})}
  \end{aligned}
  \end{equation}
  $$

- 对数似然函数：$$\ln p(X\mid\mathbf{\pi,\mu,\Sigma})=\sum^N_{n=1}\ln(\sum^K_{k=1}\pi_k\cdot\mathcal{N}(\mathbf{x_n}\mid\mathbf{\mu_k,\Sigma_k}))$$ 
 
- E步：使用当前参数计算“责任”
  
  $$
  \begin{equation}
  \begin{aligned}
  \gamma(z_{nk})=\frac{\pi_k\cdot\mathcal{N}(\mathbf{x_n}\mid\mathbf{\mu_k,\Sigma_k})}{\sum^K_{j=1}\pi_j\cdot\mathcal{N}(\mathbf{x}_n\mid\mathbf{\mu_j,\Sigma_j})}
  \end{aligned}
  \end{equation}
  $$

- M步：使用当前的“责任”重新估计参数
    - 定义：$$N_k=\sum^N_{n=1}\gamma(z_{nk})$$
    - 由$$\ln p(X\mid\mathbf{\pi,\mu,\Sigma})$$对$$\mu_k$$求偏导等于0，并两侧乘上$$\Sigma_k$$整理得到
      $$\mu_k^{new}=\frac{1}{N_k}\sum^N_{n=1}\gamma(z_{nk})\mathbf{x}_n$$
    
    - 由$$\ln p(X\mid\mathbf{\pi,\mu,\Sigma})$$对$$\Sigma_k$$求偏导等于0,整理得到
      $$\Sigma^{new}_k=\frac{1}{N_k}\sum^N_{n=1}\gamma(z_{nk})(\mathbf{x}_n-\mu_k^{new})(\mathbf{x}_n-\mu_k^{new})^T$$

    - 使用拉格朗日乘数法让关于$$\pi_k$$,最大化$$\ln p(X\mid\mathbf{\pi,\mu,\Sigma})$$约束条件为$$\sum^K_{k=1}\pi_k=1$$
        - 转换成无约束可得：$$\ln p(X\mid\mathbf{\pi,\mu,\Sigma})+\lambda(\sum^K_{k=1}\pi_k=1)$$
        - 然后对$$\pi_k$$求偏导等于0，可得：$$0=\sum^N_{n=1}\frac{\mathcal{N}(\mathbf{x}_n\mid\mathbf{\mu_k,\Sigma_k})}{\sum^K_{j=1}\pi_j\cdot\mathcal{N}(\mathbf{x}_n\mid\mathbf{\mu_j,\Sigma_j})}+\lambda$$
        - 将两侧乘以$$\pi_k$$以及令$$\lambda=-N$$,整理可得
          $$\pi_k^{new}=\frac{N_k}{N}$$

#### 2.4.GMM算法流程
- 1.输入：样本集$$D={x_1,x_2,...,x_n}$$,高斯混合成分个数k
- 2.循环    
    + 1.for n in N
        + 计算$$\gamma(z_{nk})$$ 
    + 2.for k in K
        + 计算新均值：$$\mu_k^{new}=\frac{1}{N_k}\sum^N_{n=1}\gamma(z_{nk})\mathbf{x}_n$$ 
        + 计算新协方差：$$\Sigma^{new}_k=\frac{1}{N_k}\sum^N_{n=1}\gamma(z_{nk})(\mathbf{x}_n-\mu_k^{new})(\mathbf{x}_n-\mu_k^{new})^T$$
        + 计算新混合系数：$$\pi_k^{new}=\frac{N_k}{N}$$
    - 3.更新模型参数
- 3.$$C_k=\varnothing(1\leq k\leq K)$$

- 4.for n in N 
    - 根据$$\lambda_k=\arg \max_{k\in\{1,2,...,K\}}\gamma(z_{nk})$$式，确定$$\mathbf{x}_n$$的簇标记$$\lambda_k$$  
    - 将$$\mathbf{x}_n$$划入相应的簇：$$C_{\lambda_k}=C_{\lambda_k}\bigcup\{\mathbf{x}_n\}$$
- 5.输出：簇划分$$C=\{C_1,C_2,...,C_K\}$$   


### 参考
- 《机器学习》周志华
- 《PRML》Christopher Bishop 
- 《概率论统计下篇》Jason博士
- [《人人都懂EM》](https://zhuanlan.zhihu.com/p/36331115)
