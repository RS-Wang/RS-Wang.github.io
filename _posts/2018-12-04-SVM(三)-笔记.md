---
layout:     post   				                            # 使用的布局（不需要改）
title:      SVM(三)				                   # 标题 
subtitle:   SVR/支持向量回归            # 副标题
date:       2018-12-04 				                        # 时间
author:     RS-Wang 						                      # 作者
header-img: img/post-bg-ml2.jpg 	                    #这篇文章标题背景图片
catalog: true 						                            # 是否归档
usemathjax: true								          
tags:                                                                 #标签
    - 机器学习
    - 监督学习
    - SVM
    - 支持向量回归
---

## 支持向量回归(SVR)
### 1.目标函数：
![](https://note.youdao.com/yws/api/personal/file/4EF6D3325DF94E4FAEBCE645255B7809?method=download&shareKey=92f207d7b34603f0cda2be31ffdd046a)

> SVR拥有容忍度$$\epsilon$$，即就是能容忍$$f(x)$$与y之间最多有$$\epsilon$$的偏差，只有$$f(x)$$与y之间的差值的绝对值大于$$\epsilon$$时才计算损失。如上图所示，这相当于以$$f(x)$$为中心，构建了一个宽度为$$2\epsilon$$的间隔带，若训练样本落入此间隔带中时，则被认为预测正确。

#### 1.1支持向量回归与传统回归模型对比：
- 1.支持向量回归在计算损失时多了容忍度$$\epsilon$$的约束
- 2.传统回归模型在计算损失时当且仅当$$f(x)$$与y完全相同时，损失才为零。而支持向量回归只有$$f(x)$$与y之间的差值的绝对值大于$$\epsilon$$时才计算损失。
- 3.支持向量回归相比传统回归模型更不容易过拟合和鲁棒性

#### 1.2SVR目标函数为：
- 1.SVR问题可形式化为：

  $$
  \begin{equation}
  \begin{aligned}
  \min_{W,b} &\frac{1}{2}\Vert W\Vert_2+C\sum^N_{i=1}\ell_{\epsilon}(f(
  \mathbf{x}_i)-y_i)
  \end{aligned}
  \end{equation}
  $$  
    
    - 其中C为正则化常数，$$\ell_{\epsilon}$$是不敏感损失函数：
      
      ![](https://note.youdao.com/yws/api/personal/file/F7CCED84B8A74C47976AF090A661F3AA?method=download&shareKey=b8992e06e03356cb2e14dfb2a128d74d)
    
      $$
      \ell_{\epsilon}(z)=\begin{equation}
      \left\{
      \begin{array}{**lr**}
       0,& if\ \mid z\mid\leq\epsilon;\\ 
       \mid z\mid-\epsilon,& otherwise    
      \end{array}
      \right.
      \end{equation}
      $$

- 2.引入松弛变量$$\xi_i,\hat{\xi_i}$$上式可重写为：
 
    $$
    \begin{equation}
    \begin{aligned}
    \min_{W,b,\xi_i,\hat{\xi_i}} &\frac{1}{2}\Vert W\Vert_2+C\sum^N_{i=1}(\xi_i+\hat{\xi_i})\\
    s.t. \ & f(x_i)-y_i\leq\epsilon+\xi_i,\\
    &y_i-f(x_i)\leq\epsilon+\hat{\xi_i}, \\
    &\xi_i\geq0,\hat{\xi_i}\geq0,i=1,2,...,N
    \end{aligned}
    \end{equation}
    $$

### 2.拉格朗函数：

$$
\begin{equation}
\begin{aligned}
L(\mathbf{W,b,\xi,\hat{\xi},\lambda,\hat{\lambda},\mu,\hat{\mu}})& =\frac{1}{2}\Vert W\Vert_2+C\sum^N_{i=1}(\xi_i+\hat{\xi_i})-\sum^N_{i=1}\mu_i\xi_i-\sum^N_{i=1}\hat{\mu_i}\hat{\xi_i} \\
& +\sum^N_{i=1}\lambda_i(f(x_i)-y_i-\epsilon-\xi_i)+\sum^N_{i=1}\hat{\lambda_i}(y_i-f(x_i)-\epsilon-\hat{\xi_i})
\end{aligned}
\end{equation}
$$

### 3.对偶函数：
#### 3.1求令$${\mathbf{W,b}}L(\mathbf{W,b,\xi,\hat{\xi},\lambda,\hat{\lambda},\mu,\hat{\mu}})$$对$$W,b,\xi_i,\hat{\xi_i}$$求偏导为零可得：

$$
\begin{equation}
\begin{aligned}
W &=\sum^N_{i=1}(\hat{\lambda_i}-\lambda_i)\mathbf{x}_i\\
0 &=\sum^N_{i=1}(\hat{\lambda_i}-\lambda_i)\\
C &=\lambda_i+\mu_i\\
C &=\hat{\lambda_i}+\hat{\mu_i}
\end{aligned}
\end{equation}
$$

#### 3.2对偶函数：

$$
\begin{equation}
\begin{aligned}
\max_{\mathbf{\lambda,\hat{\lambda}}} &\sum^N_{i=1}y_i(\hat{\lambda_i}-\lambda_i)-\epsilon(\hat{\lambda_i}-\lambda_i)-\frac{1}{2}\sum^N_{i=1}\sum^N_{j=1}(\hat{\lambda_i}-\lambda_i)(\hat{\lambda_j}-\lambda_j)\mathbf{x}_i^T\mathbf{x}_j\\
s.t. \ & \sum^N_{i=1}(\hat{\lambda_i}-\lambda_i)=0\\
&0\leq\lambda_i,\hat{\lambda_i}\leq C
\end{aligned}
\end{equation}
$$

### 4.KKT:
#### 4.1KKT条件：
- 1.$$\lambda_i(f(x_i)-y_i-\epsilon-\xi_i)=0$$
- 2.$$\hat{\lambda_i}(y_i-f(x_i)-\epsilon-\hat{\xi_i})=0$$
- 3.$$\lambda_i\hat{\lambda_i}=0,\xi_i\hat{\xi_i}=0$$
- 4.$$(C-\lambda_i)\xi_i=0,(C-\hat{\lambda_i})\hat{\xi_i}=0$$

#### 4.2解释：
- 当$$f(x_i)-y_i-\epsilon-\xi_i=0$$时，$$\lambda_i$$能取非零值
- 当$$y_i-f(x_i)-\epsilon-\hat{\xi_i}=0$$时，$$\hat{\lambda_i}$$能取非零值
- $$f(x_i)-y_i-\epsilon-\xi_i=0$$与$$y_i-f(x_i)-\epsilon-\hat{\xi_i}=0$$不能同时成立
- $$\lambda_i,\hat{\lambda_i}$$中至少有一个为零
- 对每个样本$$(x_i,y_i)$$都有$$(C-\lambda_i)\xi_i=0$$且$$\lambda_i(f(x_i)-y_i-\epsilon-\xi_i)=0$$.得到$$\lambda_i$$,若$$0<\lambda_i<C$$,则必有$$\xi_i=0$$，进而得到$$b=y_i+\epsilon-\sum^N_{i=1}(\hat{\lambda_i}-\lambda_i)\mathbf{x}_i^T\mathbf{x}$$

### 5.SVR的解形式：
$$
\begin{equation}
\begin{aligned}
f(\mathbf{x})=\sum^N_{i=1}(\hat{\lambda_i}-\lambda_i)\mathbf{x}_i^T\mathbf{x}+b
\end{aligned}
\end{equation}
$$

- 其中b可以采用更加鲁棒的办法：选取多个或全部满足条件$$0<\lambda_i<C$$的样本后求解b后取平均值

### 6.SVR与核函数
- W可以表示为：$$W=\sum^N_{i=1}(\hat{\lambda_i}-\lambda_i)\phi(\mathbf{x}_i)$$
- SVR可表示为：$$f(\mathbf{x})=\sum^N_{i=1}(\hat{\lambda_i}-\lambda_i)k(\mathbf{x}_i,\mathbf{x})+b$$
    - 其中$$k(\mathbf{x}_i,\mathbf{x}_j)=\phi(\mathbf{x}_i)^T\phi(\mathbf{x}_j)$$ 
### 参考
- 《机器学习》周志华
