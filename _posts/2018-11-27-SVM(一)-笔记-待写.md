---
layout:     post   				                            # 使用的布局（不需要改）
title:      SVM(一)				                   # 标题 
subtitle:   SVC/线性可分/SMO            # 副标题
date:       2018-11-27 				                        # 时间
author:     RS-Wang 						                      # 作者
header-img: img/post-bg-ml2.jpg 	                    #这篇文章标题背景图片
catalog: true 						                            # 是否归档
usemathjax: true
tags:								                                  #标签
tags:                                                                 #标签
    - 机器学习
    - 监督学习
    - SVM
    - 线性可分 
---

## 线性可分
### 1.前期铺垫
- [对偶问题与KKT]({% post_url 2018-11-24-凸优化-笔记 %})
- 二分类：y特定取值为{-1,1}
- 样本空间中，切分超平面为：$$H=\{W^TX+b=0\}$$
    - W为法向量，b为偏移
- 空间中任意一点P到平面的距离为：
$$r=\frac{\mid W^TP+b\mid}{\Vert W\Vert_2}$$ 
> 推导过程：
> 
> ![](https://note.youdao.com/yws/api/personal/file/600ADA1680654B4EAF95C5EB0D14BCED?method=download&shareKey=15191982e51657f03174781b5c54f6c8)
> 
> P为超平面外的一点，P'是P在超平面H上的投影
> 
> $$\because P-P'\perp H$$
> 
> $$\therefore P-P'=\alpha W$$
> 
> $$
\begin{aligned}
\therefore W^TP-W^TP'&=\alpha\Vert W\Vert^2_2 \\ 
W^TP+b &=\alpha\Vert W\Vert^2_2\\
\Rightarrow \alpha &=\frac{W^TP+b}{\Vert W\Vert^2_2}
\end{aligned}
$$
> 
>$$
\begin{aligned}
\therefore \Vert P-P'\Vert_2 &=\mid\alpha\mid\Vert W\Vert_2 \\
&=\frac{\mid W^TP+b\mid}{\Vert W\Vert^2_2}\cdot\Vert W\Vert_2\\
&=\frac{\mid W^TP+b\mid}{\Vert W\Vert_2}
\end{aligned}
$$ 
> 

### 2.SVM基本型
#### 2.1.SVM目标函数
![](https://note.youdao.com/yws/api/personal/file/4D776D3D02064E499CFFE405C7EF1D7B?method=download&shareKey=fc804fb2466d30b022a307ea3bc97974)

- 几何间隔：$$M=\min_i r_i=\min_i\frac{\mid W^T\mathbf{x}_i+b\mid}{\Vert W\Vert_2}$$
- SVM目标函数:
$$
\begin{aligned}
\max_{W,b}&M\\
\max_{W,b}&\{\min_i\frac{\mid W^T\mathbf{x}_i+b\mid}{\Vert W\Vert_2}\}
\end{aligned}
$$

- 解释：找到距离最近点，然后最大化几何间隔，即找到一个超平面来尽可能的分开样本

#### 2.2.SVM基本型
- 1.因为$$y\in \{-1,1\}$$,所以$$\mid W^T\mathbf{x}_i+b\mid$$就可以改写为$$y_i(W^T\mathbf{x}_i+b)$$
- 2.目标函数就变成了：$$\max_{W,b}\{\frac{1}{\Vert W\Vert_2}\min_iy_i(W^T\mathbf{x}_i+b)\}$$
- 3.因为同时让W,b扩大k倍，点到超平面的距离不变
- 4.那么就设定距离最近的点，$$y_i(W^T\mathbf{x}_i+b)=1$$
- 5.同时有：$$y_i(W^T\mathbf{x}_i+b)\geq1,i=1,2,...,N$$
- 6.目标函数就可以写成(SVM基本型)：

$$
\begin{equation}
\begin{aligned}
\min_{W,b} &\frac{1}{2}\Vert W\Vert_2\\
s.t. &y_i(W^T\mathbf{x}_i+b)\geq1,i=1,2,...,N
\end{aligned}
\end{equation}
$$

### 3.SVM对偶问题
- #### 3.1.将SVM基本型转换成Lagrangian函数：

$$
\begin{equation}
\begin{aligned}
L(W,b,\mathbf{\lambda})=\frac{1}{2}\Vert W\Vert_2+\sum^N_{i=1}\lambda_i(1-y_i(W^T\mathbf{x}_i+b))
\end{aligned}
\end{equation}
$$

- #### 3.2.对偶问题：

$$
\begin{equation}
\begin{aligned}
d^*=\max_{\mathbf{\lambda\geq0}}g(\mathbf{\lambda})=\max_{\mathbf{\lambda\geq0}}\{\min_{W,b}L(W,b,\mathbf{\lambda})\}
\end{aligned}
\end{equation}
$$

- #### 3.3.对偶函数：
    - 1.为求出$$g(\mathbf{\lambda})$$函数，令$$L(W,b,\mathbf{\lambda})$$对W,b求偏导等于0：

    $$
    \begin{equation}
    \begin{aligned}
    \frac{\partial L(W,b,\mathbf{\lambda})}{\partial W}&=W-\sum^N_{i=1}\lambda_iy_i\mathbf{x}_i=0\\
    \Rightarrow W&=\sum^N_{i=1}\lambda_iy_i\mathbf{x}_i\\
    \frac{\partial L(W,b,\mathbf{\lambda})}{\partial b}&=-\sum^N_{i=1}\lambda_iy_i=0\\
    \Rightarrow 0&=\sum^N_{i=1}\lambda_iy_i
    \end{aligned}
    \end{equation}
    $$ 

    - 2.对偶函数：

    $$
    \begin{equation}
    \begin{aligned}
    \max_{\mathbf{\lambda\geq0}}g(\mathbf{\lambda})=\max_{\mathbf{\lambda\geq0}}&\sum^N_{i=1}\lambda_i-\frac{1}{2}\sum^N_{i=1}\sum^N_{j=1}\lambda_i\lambda_jy_iy_j\mathbf{x}_i^T\mathbf{x}_j\\
    s.t. &\sum^N_{i=1}\lambda_iy_i=0,\\
    &\lambda_i\geq0,i=1,2,...,N
    \end{aligned}
    \end{equation}
    $$

    - 假定能够解出$$\mathbf{\lambda^*}$$,强对偶，根据KKT条件有：
        - $\lambda^*_i\geq0$
        - $$y_i(W^{*T}\mathbf{x}_i+b^*)\geq1$$
        - $$\lambda^*_i(1-y_i(W^{*T}\mathbf{x}_i+b^*))=0$$
        - $$W^*=\sum^N_{i=1}\lambda^*_iy_i\mathbf{x}_i,\sum^N_{i=1}\lambda^*_iy_i=0$$
    - KKT解释：
        - 若$$\lambda^*_j=0$$,对$$W^*$$没有贡献
        - 若$$\lambda^*_j>0$$,则$$y_j(W^{*T}\mathbf{x}_j+b^*)=1，\mathbf{x}_j$$是一个支持向量
        - $$W^*=\sum^N_{i=1}\lambda^*_iy_i\mathbf{x}_i$$代入 $$y_j(W^{*T}\mathbf{x}_j+b^*)=1$$,则 $$b^*=y_j-\sum^N_{i=1}\lambda^*_iy_i(\mathbf{x}_i^T\mathbf{x}_j)$$

- #### 3.4.最优分离超平面:

    $$
    \begin{equation}
    \begin{aligned}
    W^{*T}\mathbf{x}+b^*&=0\\
    \sum^N_{i=1}\lambda^*_iy_i\mathbf{x}_i^T\mathbf{x}+b^*&=0
    \end{aligned}
    \end{equation}
    $$

- #### 3.5.决策函数:
  ![](https://note.youdao.com/yws/api/personal/file/6B3679399A6D496CB9D4E99093236459?method=download&shareKey=fd642b67a0e65d43818b7bf0ea101b89)
    
    $$
    \begin{equation}
    \begin{aligned}
    f(\mathbf{x})=sign(\sum^N_{i=1}\lambda^*_iy_i\mathbf{x}_i^T\mathbf{x}+b^*)
    \end{aligned}
    \end{equation}
    $$
    
    - 输出不是概率，而是+1或者-1，代表二分类
    - 预测输出和$$\sum^N_{i=1}\lambda^*_iy_i\mathbf{x}_i^T\mathbf{x}+b^*$$的绝对值大小无关，只和$$\sum^N_{i=1}\lambda^*_iy_i\mathbf{x}_i^T\mathbf{x}+b^*$$的符号有关
    - $$\sum^N_{i=1}\lambda^*_iy_i\mathbf{x}_i^T\mathbf{x}+b^*$$几何意义：正比于x到平面的有向距离

### 4.SMO求解$$\lambda^*$$ 
#### 4.1.SMO基本思路
先固定$$\lambda_i$$之外的所有参数，然后求$$\lambda_i$$上的极值。由于存在约束$$\sum^N_{i=1}\lambda_iy_i=0$$,若固定$$\lambda_i$$之外的其他变量，则$$\lambda_i$$可由其他变量导出。于是，SMO每次选择两个变量$$\lambda_i$$和$$\lambda_j$$,并固定其他参数。这样，在参数初始化后，SMO不断执行如下两个步骤直至收敛：

- 选取一对需更新的变量$$\lambda_i$$和$$\lambda_j$$
- 固定$$\lambda_i$$和$$\lambda_j$$以外的参数，求解对偶函数获得更新后的$$\lambda_i$$和$$\lambda_j$$

> 注意到只需选取的$$\lambda_i$$和$$\lambda_j$$中有一个不满足KKT条件，目标函数就会在迭代后减少。直观来看，KKT条件违背的程度越大，则变量更新后可能导致的目标函数值减幅越大。SMO采用了一个启发式：使选取的两个变量所对应样本之间的间隔最大。一个直观的解释是，这样的两个变量有很大的差别，与对两个相似的变量进行更新相比，对它们进行更新会带给目标函数值更大的变化。
> 

#### 4.2.求解：
- 仅考虑$$\lambda_i,\lambda_j$$时，对偶函数中的约束可重写为：$$\lambda_iy_i+\lambda_jy_j=c，\lambda_i\geq0,\lambda_j\geq0$$
    - 其中$$c=-\sum_{k\neq i,j}\lambda_ky_k$$是使$$\sum^N_{i=1}\lambda_iy_i=0$$成立的常数
- 用$$\lambda_iy_i+\lambda_jy_j=c$$消去对偶函数中的$$\lambda_j$$,则得到一个关于$$\lambda_i$$的单变量二次规划问题，仅有的约束是$$\lambda_i\geq0$$
- 这样的二次规划问题具有闭式解，不需要调用数值优化算法即可高效的计算出更新后的$$\lambda_i,\lambda_j$$

#### 4.3.$$W^*,b^*$$的值：
$$S=\{i\mid\lambda_i>0,i=1,2,...,m\}$$为所有支持向量的下标集

- $$W^*$$值：$$W^*=\sum_{i\in S}\lambda_iy_i\mathbf{x_i}$$

- $$b^*$$值：
    - 现实中常采用更鲁棒的做法求解$$b^*$$：$$b^*=\frac{1}{\mid S\mid}\sum_{s\in S}(y_s-\sum_{i\in S}\lambda_iy_i\mathbf{x_i}^T\mathbf{x_s})$$
    


### 参考
- 《机器学习》周志华
- 《优化论五部曲》Jason博士
