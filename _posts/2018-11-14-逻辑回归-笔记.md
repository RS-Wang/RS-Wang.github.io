---
layout:     post   				                            # 使用的布局（不需要改）
title:      逻辑回归				                   # 标题 
subtitle:   逻辑回归            # 副标题
date:       2018-11-14 				                        # 时间
author:     RS-Wang 						                      # 作者
header-img: img/post-bg-ml2.jpg 	                    #这篇文章标题背景图片
catalog: true 						                            # 是否归档
usemathjax: true
tags:								                                  #标签
    - 机器学习
    - 逻辑回归
    - 梯度下降
    - 监督学习
---

## 逻辑回归

### 1.线性映射关系：$$y = sigmoid(XW + b)$$
- X是(m,n)矩阵，W是(n,1)矩阵
- $$sigmoid(z_i)$$函数
    - 公式：$$sigmoid(z_i)=\frac{1}{1+e^{-z_i}}$$
    - 图形：
    > ![](https://gss0.bdstatic.com/94o3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D220/sign=dc4b35fe37a85edffe8cf921795509d8/c9fcc3cec3fdfc03f23fbf16d73f8794a5c226dc.jpg)
- 由于$$sigmoid(z_i)$$函数值[0,1]的特性,也可以看成将该样本判别成某类别的可能性(概率)
- 分类的概率表示为：
> 为了方便表示，令 $$\sigma(z_i)=sigmoid(z_i)$$，其中$$z_i = x_iW+b$$
> 
> 当y=1概率：$$P(y_i=1\vert x_i;W) = \sigma(z_i)$$ 
> 
> 当y=0概率：$$P(y_i=0\vert x_i;W) = 1-\sigma(z_i)$$
> 
> 统一上式概率：$$P(y_i\vert x_i;W) = \sigma(z_i)^y_i(1-\sigma(z_i))^{1-y_i}$$

### 2.损失函数交叉熵（最小化）:
- 一般表示：
    $$J(W) = -\frac{1}{m}\sum^m_{i=1}(y_i \ln(\sigma(x_iW))+(1-y_i) \ln(1-\sigma(x_iW)))$$
- 向量化表示：
    $$J(W) = -\frac{1}{m}(\ln(\sigma(XW))^Ty+\ln(1-\sigma(XW)^T(1-y))) $$

### 3.损失函数由来：
- 极大似然估计思想：
> 1.似然函数：
>
    $$
    \begin{equation}
    \begin{aligned}
    L(W) &=\Pi^m_{i=1}P(y_i\vert x_i;W)\\
    &=\Pi^m_{i=1}\sigma(x_iW)^{y_i}(1-\sigma(x_iW))^{1-y_i}
    \end{aligned}
    \end{equation}
    $$
>
> 2.对似然函数进行取对数,令 $$H(W) = \ln L(W)$$
>
    $$
    \begin{equation}
    \begin{aligned}
    H(W) &=\ln L(W)\\
    &=\ln (\Pi^m_{i=1}\sigma(x_iW)^{y_i}(1-\sigma(x_iW))^{1-y_i})\\
    &=\sum^m_{i=1}(y_i \ln(\sigma(x_iW))+(1-y_i) \ln(1-\sigma(x_iW)))\\
    \end{aligned}
    \end{equation}
    $$
>
> 3.令 $$J(W) = - \frac{1}{m}H(W) = -\frac{1}{m}\sum^m_{i=1}(y_i \ln(\sigma(x_iW))+(1-y_i) \ln(1-\sigma(x_iW)))$$
> 
> 4.则$$\mathop{\arg\max}_{W} H(w) \Leftrightarrow \mathop{\arg\min}_{W}J(W)$$

### 4.求解$$\mathop{\arg\min}_{W}J(W)$$
- #### 1.$$sigmoid(z_i)$$对$$z_i$$求导：
    $$
    \begin{equation}
    \begin{aligned}
    \frac{\partial\sigma(z_i)}{\partial z_i} &= \frac{e^{-z_i}}{(1+e^{-z_i})^2}\\
    &=\frac{1+e^{-z_i}-1}{(1+e^{-z_i})^2}\\
    &=\frac{1}{1+e^{-z_i}}-\frac{1}{(1+e^{-z_i})^2}\\
    &=\sigma(z_i)(1-\sigma(z_i))
    \end{aligned}
    \end{equation}
    $$
- #### 2.$$sigmoid(z_i)$$对W求导：
    $$
    \begin{equation}
    \begin{aligned}
    \frac{\partial\sigma(z_i)}{\partial W} &= \frac{\partial\sigma(z_i)}{\partial z_i}\cdot\frac{\partial z_i}{\partial W}\\
    & = \sigma(z_i)(1-\sigma(z_i))x_i
    \end{aligned}
    \end{equation}
    $$
- #### 3.使用梯度下降法求W：
    - 3.1令 $$g(W)=\frac{\partial J(z_i)}{\partial W}$$,即：
    $$
    \begin{equation}
    \begin{aligned}
    g(W)&=\frac{\partial J(W)}{\partial W}\\
    &=-\frac{1}{m}\sum^m_{i=1}(y_i\frac{\partial\ln\sigma(z_i)}{\partial\sigma(z_i)}\cdot\frac{\partial\sigma(z_i)}{\partial z_i}\cdot\frac{\partial z_i}{\partial W}+(1-y_i)\frac{\partial\ln(1-\sigma(z_i))}{\partial(1-\sigma(z_i))}\cdot\frac{\partial(1-\sigma(z_i))}{\partial z_i}\cdot\frac{\partial z_i}{\partial W})\\
    &=-\frac{1}{m}\sum^m_{i=1}(y_i\frac{1}{\sigma(z_i)}\cdot\sigma(z_i)(1-\sigma(z_i))x_i +(1-y_i)\frac{1}{1-\sigma(z_i)}\cdot-\sigma(z_i)(1-\sigma(z_i))x_i)\\
    &=-\frac{1}{m}\sum^m_{i=1}(y_i(1-\sigma(z_i))-(1-y_i)\sigma(z_i))x_i\\
    &=-\frac{1}{m}\sum^m_{i=1}(y_i-\sigma(z_i))x_i\\
    &=\frac{1}{m}\sum^m_{i=1}(\sigma(z_i)-y_i)x_i
    \end{aligned}
    \end{equation}
    $$
    - 3.2$$g(W)$$向量化表示：
    $$ g(W)=\frac{1}{m}X^T(\sigma(XW)-y)$$
    - 3.3更新W直到$$\parallel g(W)\parallel_2$$小于容忍度为止：
    $$W^{new}=W^{old}-\alpha g(W^{old}) $$

### 参考
- 《机器学习》周志华
