---
layout:     post   				                            # 使用的布局（不需要改）
title:      降维				                   # 标题 
subtitle:   低维嵌入/PCA/流行学习            # 副标题
date:       2018-12-18 				                        # 时间
author:     RS-Wang 						                      # 作者
header-img: img/post-bg-ml2.jpg 	                    #这篇文章标题背景图片
catalog: true 						                            # 是否归档
usemathjax: true
tags:								                                  #标签
tags:                                                                 #标签
    - 机器学习
    - 低维嵌入
    - PCA
    - 流行学习
    - 无监督学习
---

### 1.低维嵌入
#### 1.1.MDS
> 原始空间中样本之间的距离在低维空间中得以保持

- 设定：
    + 假定m个样本在原始空间的距离矩阵为$$D\in\mathbb{R}^{m\mathrm{x}m}$$,其中第i行j列的元素$$dist_{ij}$$为样本$$x_i$$与$$x_j$$的距离。
    + 假定样本在$$d'$$低维空间的表示$$Z\in\mathbb{R}^{d'\mathrm{x}m}$$,$$d'\leq d$$
    + $$\Vert z_i-z_j\Vert_2=dist_{ij}$$.
    + 令$$B=Z^TZ\in\mathbb{R}^{m\mathrm{x}m}$$,B为降维后样本内积矩阵，$$b_{ij}=z_i^Tz_j$$ 
    $$
    \begin{equation}
    \begin{aligned}
    dist_{ij}^2&=\Vert z_i\Vert^2_2+\Vert z_j\Vert^2_2-2z_i^Tz_j\\
    &=b_{ii}+b_{jj}-2b_{ij}
    \end{aligned}
    \end{equation}
    $$

- 求解B矩阵：
    + 令降维后样本Z被中心化，即
        + $$\sum^m_{i=1}z_i=0$$,
        + $$\sum^m_{i=1}b_{ij}=\sum^m_{j=1}b_{ij}=0$$,
        + $$\sum^m_{i=1}dist_{ij}^2=tr(B)+mb_{jj}$$,
        + $$\sum^m_{j=1}dist_{ij}^2=tr(B)+mb_{ii}$$,
        + $$\sum^m_{i=1}\sum^m_{j=1}dist_{ij}^2=2mtr(B)$$.
    + tr(*),表示矩阵的迹,$$tr(B)=\sum^m_{i=1}\Vert z_i\Vert^2$$
        + $$dist_{i*}^2=\frac{1}{m}\sum_{j=1}^mdist^2_{ij}$$,
        + $$dist_{*j}^2=\frac{1}{m}\sum_{i=1}^mdist^2_{ij}$$,
        + $$dist_{**}^2=\frac{1}{m^2}\sum_{i=1}^m\sum_{j=1}^mdist^2_{ij}$$, 
    + 由上式可得：
    $$
    \begin{equation}
    \begin{aligned}
    b_{ij}=-\frac{1}{2}(dist_{ij}^2-dist_{i*}^2-dist_{*j}^2+dist_{**}^2)
    \end{aligned}
    \end{equation}
    $$

    + 可通过降维前后保持不变的距离矩阵D求取内积矩阵B

- 求解矩阵Z：
    - 对矩阵B做特征分解，$$B=V\Lambda V^T$$ 
    - 因为$$B=Z^TZ\in\mathbb{R}^{m\mathrm{x}m}$$,所以$$Z=\Lambda^{1/2}V^T\in\mathbb{R}^{d'\mathrm{x}m}$$


### 2.PCA
> PCA 是线性降维方法

#### 2.1.优化目标

$$
\begin{equation}
\begin{aligned}
\min_W & \Vert X-XW^TW\Vert^2_F\\
s.t. & WW^T=I_p
\end{aligned}
\end{equation}
$$

- W是正交矩阵

#### 2.2.求解矩阵W
![](https://note.youdao.com/yws/api/personal/file/29C55BFF0D7446779909B33005E17210?method=download&shareKey=5b6c2a13496e54a3c1c026194c3cc98e)

- 1.假设P=1(先求解第一方向)，优化目标变成

$$
\begin{equation}
\begin{aligned}
\min_w & \Vert X-Xww^T\Vert^2_F\\
s.t. & w^Tw=1
\end{aligned}
\end{equation}
$$

- 2.化简
> $$
\begin{equation}
\begin{aligned}
\Vert X-Xww^T\Vert^2_F &=tr((X-Xww^T)^T(X-Xww^T))\\
&=tr(X^TX-X^TXww^T-ww^TX^TX+ww^TX^TXww^T)\\
&\varpropto tr(-2X^TXww^T+ww^TX^TXww^T)\\
&=-2tr(w^TX^TXw)+tr(w^Tww^TX^TXw)\\
&=-tr(w^TX^TXw)
\end{aligned}
\end{equation}
$$
> 
>    - 1.$$\Vert A\Vert_F^2=tr(A^TA)$$
>    - 2.$$tr(A+B)=tr(A)+tr(B)$$
>    - 3.$$tr(AB)=tr(BA)$$

    - 由上面公式可知，优化目标可替换为：
    
      $$
      \begin{equation}
      \begin{aligned}
      \max_w & w^TX^TXw\\
      s.t. & w^Tw=1
      \end{aligned}
      \end{equation}
      $$

- 3.求解替换后的优化目标
    - 1.约束问题：
    
    $$
    \begin{equation}
    \begin{aligned}
    \max_w & w^TCw\\
    s.t. & w^Tw=1
    \end{aligned}
    \end{equation}
    $$
        - 其中：$$C=X^TX\in\mathbb{R}^{d\mathrm{x}d}$$是协方差矩阵（样本已经中心化）
        
    - 2.拉格朗日函数：$$L(w,\lambda)=w^TCw-\lambda w^Tw+\lambda$$
    - 3.求$$\frac{\partial L}{\partial w}=0$$：$$\frac{\partial L}{\partial w}=Cw-\lambda w=0$$
    - 4.可得：$$Cw=\lambda w$$

- 4.选取$$d'$$个特征值对应的特征向量为所求的w,选取标准：$$\frac{\sum^{d'}_{i=1}\lambda_i}{\sum^{d}_{i=1}\lambda_i}\geq t$$
    - 其中：例如t=95%
- 5.输入降维后的样本: $$Z=XW^T$$

### 3.流行学习
> 流行学习是一类借鉴了拓扑流行概念的降维方法."流形"是在局部与欧氏空间同胚的空间，换言之，它在局部具有欧氏空间的性质，能用欧氏距离来进行距离计算。
#### 3.1.等度量映射(Isomap)
![](https://note.youdao.com/yws/api/personal/file/D1D4ADA68F7E4E54B09C8D98F9CF6D98?method=download&shareKey=3c91190cefba56e0820e3b7f2ef057b3)

- 主要想法：用局部的欧氏距离来近似局部测地距离
- 算法流程：
    - 循环遍历每个样本确定$$x_i$$的k近邻，并计算$$x_i$$与k近邻点之间的距离设置为欧氏距离，与其他点的距离设置为无穷大
    - 使用Dijkstra算法或Floyd算法计算任意两样本点之间的距离$$dist(x_i,x_j)$$
    - 将$$dist(x_i,x_j)$$作为MDS算法的输入
    - 输出MDS算法的解矩阵Z

- 不足：
    - 新样本：
        - Isomap仅是得到了训练样本在低维空间的坐标，对于新样本，只能将训练样本的高维空间坐标作为输入，低维空间坐标作为输出，训练出一个回归学习器对新样本的低维空间坐标进行预测
    - 构建近邻：
        - 构建近邻有两种方式，一种指定近邻点数如K近邻，另一种是指定距离阈值$$\epsilon$$,距离小于阈值就认为近邻点
        - 这两种方式均有不足，若近邻范围指定大了，会让距离很远的点误认为近邻，出现“短路”问题。若范围指定小了，则图中有些区域可能与其他区域不存在连接，出现“断路”。
        - 短路与断路都会给后续的最短路径计算造成误导。



### 参考
- 《机器学习》周志华
- 《数据降维艺术》Jason博士
