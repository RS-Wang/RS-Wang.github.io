---
layout:     post   				                            # 使用的布局（不需要改）
title:      无约束优化迭代法				                   # 标题 
subtitle:   梯度下降/牛顿法/拟牛顿法                      # 副标题
date:       2018-11-17 				                        # 时间
author:     RS-Wang 						                      # 作者
header-img: img/post-bg-cook.jpg 	                    #这篇文章标题背景图片
catalog: true 						                            # 是否归档
usemathjax: true  # 是否使用LaTeX公式
tags:								                                  #标签
    - 机器学习
    - 梯度下降
    - 牛顿法
    - 拟牛顿法
    - 数学基础
---

### 1.泰勒级数与极值
- #### 1.1 标量
    - ##### 1.1.1 泰勒级数展开式：
    $$ f(x_k + \delta) \approx f(x_k)+f^{'}(x_k)\delta +\frac{1}{2} f^{''}(x_k)\delta^2+...+\frac{1}{k!}f^{k}(x_k)\delta^k+...  $$ 
    - ##### 1.1.2 极值：
        - 严格局部极小值指：$$f(x_k + \delta)>f(x_k)$$ 
        - 称满足$$f^{'}(x_k)=0$$的点为平稳点(候选点)
        - 函数在$$x_k$$ 有严格局部极小值条件为$$f^{'}(x_k)=0$且$f^{''}(x_k)>0$$ 
- #### 1.2 向量
    - ##### 1.2.1 泰勒级数展开式： 
    $$ f(X_k + \mathbf{\delta}) \approx f(X_k)+g^T(X_k)\mathbf{\delta} +\frac{1}{2}\delta^T H(X_k)\delta $$
    > 其中 $$\mathbf{\delta}$$ 是向量，$$g(X_k),H(X_k)$$分别是$$f(X_k)$$的梯度和海森矩阵
    - #### 1.2.2 极值：
        - 称满足$$g(X_k)=0$$的点为平稳点(候选点)
        - 函数满足$$g(X_k)=0$$情况下
            - 若 $$H(X_k)\succ 0$$ ，则称$$X_k$$是局部极小点(反之，严格局部最大点)
            - 若 $$H(X_k)$$ 是不定矩阵，则是鞍点
- 一般只用到二阶的泰勒级数展开来近似原函数

### 2.迭代法的基本结构(最小化f(x))
- 1.选择一个初始点，设置一个容忍度 $$\epsilon$$ ,计数k=0
- 2.决定搜索方向$$\mathbf{d_k}$$,使得函数下降(核心)
- 3.决定步长$$\alpha_k$$使得$$f(X_k+\alpha_k\mathbf{d_k})$$对于$$\alpha_k\ge 0$$最小化，构建$$X_{k+1} = X_k +\alpha_k\mathbf{d_k}$$
- 4.如果$$\parallel \mathbf{d_k}\parallel_2 <\epsilon$$,则停止输出解$$X_{k+1}$$;否则继续重复迭代

### 3.梯度下降法 
- #### 3.1泰勒级数展开式:
$$ f(X_k+\mathbf{d_k})= f(X_k)+g^T(X_k)\mathbf{d_k}$$ 
> 梯度下降法只用了函数泰勒级数展开式的一阶来近似原函数
- #### 3.2$$\mathbf{d_k}=-g(X_k)$$的原因：
    - 想要$$f(X_k+\mathbf{d_k})\downarrow$$ ，则需要$$f(X_k)$$加个负数（这个负数越小越好）
    - 因为两个向量的内积 $$\mathbf{a}\cdot\mathbf{b} =\mathbf{a}^T\mathbf{b}=\parallel \mathbf{a}\parallel_2\parallel\mathbf{b}\parallel_2\cos\theta$$
    - 所以$$\mathbf{d_k}=-g(X_k)$$，使得$$g^T(X_k)\mathbf{d_k}$$值为最小负数
- #### 3.3更新权重：
$$W_{new} = W_{old}-\alpha g(W_{old})$$

### 4.牛顿法
- #### 4.1泰勒级数展开式：
$$ f(X_k+\mathbf{d_k})=f(X_k)+g^T(X_k)\mathbf{d_k}+\frac{1}{2}\mathbf{d_k}^TH(X_k)\mathbf{d_k}$$
> 牛顿法用了泰勒级数展开式的二阶来近似原函数，因此在最小化时比梯度下降要快
- #### 4.2$$\mathbf{d_k}=-H^{-1}(X_k)g(X_k)$$的原因：
    - 令$$\frac{\partial{f(X_k+\mathbf{d_k})}}{\partial{\mathbf{d_k}}}=0 \Rightarrow g(X_k)+H(X_k)\mathbf{d_k}=0$$
    - 若海森矩阵正定，则有$$\mathbf{d_k}=-H^{-1}(X_k)g(X_k)$$
    - 强制海森矩阵为正定矩阵，才能保证$$\mathbf{d_k}$$的方向为负梯度方向和可以求逆
- #### 4.3缺陷：
    - 需要求取海森矩阵和海森矩阵的逆，工程复杂的高
    - 海森矩阵的逆不稳定

### 5.拟牛顿法(DFP)
- #### 5.1 搜索方向：$$d_k=-S_kg_k$$
- #### 5.2 核心思想：
    - 使用函数的一正定矩阵来逼近海森矩阵的逆(一阶的量慢慢近似二阶的量)
    > 这样就可以不用求难求的海森矩阵和海森矩阵的逆
    - 令$$S_k = H^{-1}(X_k)$$
    - 定义$$\mathbf{\delta_k}=X_{k+1}-X_k,\mathbf{\gamma_k} = g_{k+1}-g_k$$ 
    - 需要$$S_{k+1}\mathbf{\gamma_k} =\mathbf{\delta_k}$$
        
        > 这个是不严谨的推导只用于理解
        > 在微积分中一阶导的本质是$$f^{'}(x)=\lim_{\Delta x\to 0}\frac{f(x)-f(x_0)}{x-x_0}$$
        > 
        > 那么二阶导是$$f^{''}(x)=\lim_{\Delta x\to 0}\frac{f^{'}(x)-f^{'}(x_0)}{x-x_0}$$
        > 
        > 所以$$H(X_k) =\frac{\mathbf{\gamma_k}}{\mathbf{\delta_k}}$$
        > 
        > 所以$$H^{-1}(X_k) =\frac{\mathbf{\delta_k}}{\mathbf{\gamma_k}}$$
        > 
        > 所以$$S_{k+1}\mathbf{\gamma_k} =\mathbf{\delta_k}$$
    
    - 但是，只有$$\delta_k$$和$$\gamma_k$$，是计算不出$$S_{k+1}$$的，继续用迭代方法

- #### 5.3 DFP
    - 给定初始:$$S_0=I$$
    - 设:$$S_{k+1} = S_k +\Delta S_k,k=0,1,...$$
    - 令:$$\Delta S_k=\alpha\mathbf{uu}^T+\beta \mathbf{vv}^T$$
    > 这里$$\alpha,\beta$$为待定系数，$$\mathbf{u,v}\in\mathbb{R}^N$$为待定向量，因为$$\mathbf{uu}^T$$和$$\mathbf{vv}^T$$是对称矩阵,从而保证$$\Delta S_k$$是对称矩阵
    > 
    - 因此:
    $$\begin{equation}\begin{aligned}S_{k+1}=S_k+\alpha\mathbf{uu}^T+\beta\mathbf{vv}^T\end{aligned}\end{equation}$$
    - 两边乘以$$\gamma_k$$,可得：
    $$    
    \begin{equation}
    \begin{aligned}
    \mathbf{\delta_k} &= S_k\mathbf{\gamma_k}+\alpha\mathbf{u^T\gamma_k u}+\beta\mathbf{v^T\gamma_k v}
    \end{aligned}
    \end{equation}
    $$ 
    - 令 $$\alpha\mathbf{u^T\gamma_k}=1$$和$$\beta\mathbf{v^T\gamma_k}=-1$$带入(1)式,可得： 
    $$    
    \begin{equation}
    \begin{aligned}
    \mathbf{\delta_k} &= S_k\mathbf{\gamma_k}+\mathbf{u-v}
    \end{aligned}
    \end{equation}
    $$
        - 且：
        $$
        \begin{equation}
        \begin{aligned}
        \alpha=\frac{1}{\mathbf{u^T\gamma_k}}，\beta=-\frac{1}{\mathbf{v^T\gamma_k}}
        \end{aligned}
        \end{equation}
        $$
    - 将(3)式变换可得：
    $$    
    \begin{equation}
    \begin{aligned}
    \mathbf{u-v} &=\mathbf{\delta_k}-S_k\mathbf{\gamma_k}
    \end{aligned}
    \end{equation}
    $$   
    - 不妨令: $$\begin{equation}
        \begin{aligned}\mathbf{u}=\mathbf{\delta_k},\mathbf{v}=S_k\mathbf{\gamma_k}\end{aligned}
        \end{equation}
        $$
    - 将(4)(6)带入(1)中，可得：
    $$    
    \begin{equation}
    \begin{aligned}
    S_{k+1}=S_k+\frac{\mathbf{\delta_k\delta_k}^T}{\mathbf{\delta_k^T\gamma_k}}-\frac{S_k\mathbf{\gamma_k}\mathbf{\gamma_k}^TS_k}{\mathbf{\gamma_k^TS_k\gamma_k}}
    \end{aligned}
    \end{equation}
    $$

### 参考
- 《优化论五部曲》Jason博士
