---
layout:     post   				                            # 使用的布局（不需要改）
title:      SVM(二)				                   # 标题 
subtitle:   SVC/线性不可分/核函数            # 副标题
date:       2018-11-30 				                        # 时间
author:     RS-Wang 						                      # 作者
header-img: img/post-bg-ml2.jpg 	                    #这篇文章标题背景图片
catalog: true 						                            # 是否归档
usemathjax: true								                                  #标签
tags:                                                                 #标签
    - 机器学习
    - 监督学习
    - SVM
    - 线性不可分 
---

## 线性不可分
### 1.核函数
#### 1.1.问题：
在SVM(一)中讨论的问题都是假设训练样本是线性可分的，即存在一个划分超平面能够将训练样本正确分类，然而在现实任务中，原始样本空间也许并不存在一个正确划分两类样本的超平面。

#### 1.2.解决方法：
![](https://note.youdao.com/yws/api/personal/file/78510BA825AB47A3AE3B806C92C4D895?method=download&shareKey=accc107cefc10d7e567d3397bc0742de)

>对线性不可分的样本，可以将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分，如上图，若将原始的二维空间映射到一个合适的三维空间，就能找到一个合适的划分超平面。如果原始空间是有限维，即属性数有限，那么一定存在一个高维特征空间使样本可分。

- 1.令$$\phi(\mathbf{x})$$表示 x 映射到高维特征空间的特征向量
- 2.那么在高维特征空间中的样本就是线性可分的
    - 2.1.在特征空间中划分超平面所对应的模型可表示为：
    
    $$f(\mathbf{x})=W^T\phi(\mathbf{x})+b$$ 

    - 2.2.此时，SVM基本型：
    
    $$
    \begin{equation}
    \begin{aligned}
    \min_{W,b} &\frac{1}{2}\Vert W\Vert_2\\
    s.t. &y_i(W^T\phi(\mathbf{x}_i)+b)\geq1,i=1,2,...,N
    \end{aligned}
    \end{equation}
    $$

    - 2.3.对偶函数：
    
    $$
    \begin{equation}
    \begin{aligned}
    \max_{\mathbf{\lambda\geq0}}g(\mathbf{\lambda})=\max_{\mathbf{\lambda\geq0}}&\sum^N_{i=1}\lambda_i-\frac{1}{2}\sum^N_{i=1}\sum^N_{j=1}\lambda_i\lambda_jy_iy_j\phi(\mathbf{x}_i)^T\phi(\mathbf{x}_j)\\
    s.t. &\sum^N_{i=1}\lambda_iy_i=0,\\
    &\lambda_i\geq0,i=1,2,...,N
    \end{aligned}
    \end{equation}
    $$

    - [上面的公式由来详细请看]({% post_url 2018-11-27-SVM(一)-笔记 %})

#### 1.3.核函数：
- 1.问题：
>在求解上面对偶函数时，涉及到计算$$\phi(\mathbf{x}_i)^T\phi(\mathbf{x}_j)$$,这是样本$$\mathbf{x}_i$$与$$\mathbf{x}_j$$映射到特征空间之后的内积，由于特征空间维数可能很高，甚至可能是无穷维，因此直接计算$$\phi(\mathbf{x}_i)^T\phi(\mathbf{x}_j)$$通常是非常困难

- 2.解决方法：
    - 1.为了解决上面的问题，设想这样一个函数：$$k(\mathbf{x}_i,\mathbf{x}_j)=\langle\phi(\mathbf{x}_i),\phi(\mathbf{x}_j) \rangle=\phi(\mathbf{x}_i)^T\phi(\mathbf{x}_j)$$ 
    > 即$$\mathbf{x}_i$$与$$\mathbf{x}_j$$在特征空间的内积等于他们在原始样本空间中通过函数$$k(.,.)$$计算的结果。这样就不需要计算无穷维特征空间中的内积。
    
    - 2.上面的对偶函数可以改写为：
    
    $$
    \begin{equation}
    \begin{aligned}
    \max_{\mathbf{\lambda\geq0}}g(\mathbf{\lambda})=\max_{\mathbf{\lambda\geq0}}&\sum^N_{i=1}\lambda_i-\frac{1}{2}\sum^N_{i=1}\sum^N_{j=1}\lambda_i\lambda_jy_iy_jk(\mathbf{x}_i,\mathbf{x}_j)\\
    s.t. &\sum^N_{i=1}\lambda_iy_i=0,\\
    &\lambda_i\geq0,i=1,2,...,N
    \end{aligned}
    \end{equation}
    $$

    - 3.求解后得到：
    
    $$
    \begin{equation}
    \begin{aligned}
    f(\mathbf{x})&=W^T\phi(\mathbf{x})+b\\
    &=\sum^N_{i=1}\lambda_iy_i\phi(\mathbf{x}_i)^T\phi(\mathbf{x})+b\\
    &=\sum^N_{i=1}\lambda_iy_ik(\mathbf{x}_i,\mathbf{x})+b
    \end{aligned}
    \end{equation}
    $$

- 3.核函数：
    - 上面假设的$$k(.,.)$$函数就是核函数
    - 常用的核函数：
        - 线性核：$$k(\mathbf{x}_i,\mathbf{x}_j)=\mathbf{x}_i^T\mathbf{x}_j$$
        - 多项式核：$$k(\mathbf{x}_i,\mathbf{x}_j)=(\mathbf{x}_i^T\mathbf{x}_j)^d$$ 
        - 高斯核(RBF)：$$k(\mathbf{x}_i,\mathbf{x}_j)=\exp(-\frac{\Vert\mathbf{x}_i-\mathbf{x}_j\Vert^2}{2\sigma^2})$$
        - 拉普拉斯核：$$k(\mathbf{x}_i,\mathbf{x}_j)=\exp(-\frac{\Vert\mathbf{x}_i-\mathbf{x}_j\Vert}{\sigma})$$
        - Sigmoid核：$$k(\mathbf{x}_i,\mathbf{x}_j)=tanh(\beta\mathbf{x}_i^T\mathbf{x}_j+\theta)$$
    - 组合核函数：
        - 若$$k_1$$和$$k_2$$为核函数，则对于任意正数$$a_1,a_2$$,其线性组合：$$a_1k_1+a_2k_2$$是核函数
        - 若$$k_1$$和$$k_2$$为核函数，则核函数的直积：$$k_1\otimes k_2(\mathbf{x,z})=k_1(\mathbf{x,z})k_2(\mathbf{x,z})$$是核函数
        - 若$$k_1$$和$$k_2$$为核函数，则对于任意函数$$g(\mathbf{x})$$：$$k(\mathbf{x,z})=g(\mathbf{x})k_1(\mathbf{x,z})g(\mathbf{z})$$是核函数

    - 结论：
        - 1.样本只有找到合适的核函数才能线性可分 
        - 2.然而找到合适的核函数是困难的，需要一定经验
        - 3.若核函数选择不合适，很有可能导致性能不佳
        - 3.对文本数据通常采用线性核
        - 4.情况不明时可先尝试高斯核

### 2.软间隔    
#### 2.1.问题：
前面的讨论，都是假定训练样本在样本空间或特征空间中是线性可分的。然而，在现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分；即使恰好找到某个核函数使训练集在特征空间中线性可分，也很难断定这个貌似线性可分的结果是不是过拟合所造成的。

#### 2.2.解决方法：
![](https://note.youdao.com/yws/api/personal/file/5BBB61B1AC0B40CF93E5CE990E020CE1?method=download&shareKey=ec3202ae037bcabd59a841b22a5231a4)

> 缓解上述问题的一个办法是允许SVM在一些样本上出错，为此引入‘软间隔’。
>
> 硬间隔：所有样本都满足 $$y_i(W^T\mathbf{x}_i+b)\geq1$$这个约束条件，即所有样本都必须划分正确。
> 
> 软间隔：允许某些样本不满足$$y_i(W^T\mathbf{x}_i+b)\geq1$$约束条件。

- 1.此时优化目标可写为：
$$
\begin{equation}
\begin{aligned}
\min_{W,b} &\frac{1}{2}\Vert W\Vert_2+C\sum^N_{i=1}\ell_{0/1}(y_i(W^T\mathbf{x}_i+b)-1)
\end{aligned}
\end{equation}
$$

    - 其中C>0是一个常数，$$\ell_{0/1}$$是“0/1损失函数”：
    
    $$
    \ell_{0/1}(z)=\begin{equation}
    \left\{
    \begin{array}{**lr**}
     1,& if\ z<0;\\ 
     0,& otherwise    
    \end{array}
    \right.
    \end{equation}
    $$

    - 当C无穷大时，上式会迫使所有样本均满足约束$$y_i(W^T\mathbf{x}_i+b)\geq1$$
    - 当C取有限值时，上式允许一些样本不满足约束$$y_i(W^T\mathbf{x}_i+b)\geq1$$
    
    > $$\ell_{0/1}$$是非凸、非连续，使得上式不易求解.这时通常用其他一些函数来代替$$\ell_{0/1}$$.

- 2.三种常用的替代损失函数： 
    
    ![](https://note.youdao.com/yws/api/personal/file/F8631F8C65C14EA48952C636788B4460?method=download&shareKey=86a284ed8cff343208a254b61609512d)
    
    - hinge损失：$$\ell_{hinge}(z)=max(0,1-z)$$
    - 指数损失：$$\ell_{exp}(z)=exp(-z)$$
    - log损失：$$\ell_{log}(z)=log(1+exp(-z))$$ 
     
- 3.选择hinge损失函数：
    - 1.选择这个函数原因：
        - hinge损失有一块“平坦”的零区域，这使SVM具有稀疏解
        - log损失是有光滑的单调递减函数，不能导出稀疏解，因此log回归的解依赖更多的训练样本，其开销跟大
    - 2.采用hinge损失，优化目标变成：
    
	  $$
    \begin{equation}
    \begin{aligned}
    \min_{W,b} &\frac{1}{2}\Vert W\Vert_2+C\sum^N_{i=1}\max(0,1-y_i(W^T\mathbf{x}_i+b))
    \end{aligned}
    \end{equation}
    $$

    - 3.引入'松弛变量'$$\xi_i\geq0$$,上式可重写为：
    
	  $$
    \begin{equation}
    \begin{aligned}
    \min_{W,b,\xi_i} &\frac{1}{2}\Vert W\Vert_2+C\sum^N_{i=1}\xi_i\\
    s.t. \ &y_i(W^T\mathbf{x}_i+b)\geq1-\xi_i\\
    & \xi_i\geq0,i=1,2,...,N
    \end{aligned}
    \end{equation}
    $$

    - 4.拉格朗日函数：
    
	  $$
    \begin{equation}
    \begin{aligned}
    L(\mathbf{W,b,\lambda,\xi,\mu})=&\frac{1}{2}\Vert W\Vert_2+C\sum^N_{i=1}\xi_i\\
    &+\sum^m_{i=1}\lambda_i(1-\xi_i-y_i(W^T\mathbf{x}_i+b))-\sum^m_{i=1}\mu_i\xi_i
    \end{aligned}
    \end{equation}
    $$

        - 其中$$\lambda_i\geq0,\mu_i\geq0$$是拉格朗日乘子
    
    - 5.对偶函数：
        - 令$$L(\mathbf{W,b,\lambda,\xi,\mu})$$对$$W,b,\xi_i$$的偏导为零，可得：
        
		    $$
        \begin{equation}
        \begin{aligned}
        W=&\sum^N_{i=1}\lambda_iy_i\mathbf{x}_i,\\
        0=&\sum^N_{i=1}\lambda_iy_i,\\
        C=&\lambda_i+\mu_i
        \end{aligned}
        \end{equation}
        $$

        - 对偶函数：
        
		    $$
        \begin{equation}
        \begin{aligned}
        \max_{\mathbf{\lambda}}\ &\sum^N_{i=1}\lambda_i-\frac{1}{2}\sum^N_{i=1}\sum^N_{j=1}\lambda_i\lambda_jy_iy_j\mathbf{x}_i^T\mathbf{x}_j\\
        s.t.\ &\sum^N_{i=1}\lambda_iy_i=0,\\
        &0\leq\lambda_i\leq C,i=1,2,...,N.
        \end{aligned}
        \end{equation}
        $$

    - 6.KKT条件：
        - 上式与硬间隔下的对偶函数唯一不同就是对偶变量约束不同:前者$$0\leq\lambda_i\leq C$$,后者$$0\leq\lambda_i$$
        - KKT条件：
            - 1.$$\lambda_i\geq0,\mu_i\geq0$$
            - 2.$$y_if(\mathbf{x_i})-1+\xi_i\geq0$$
            - 3.$$\lambda_i(y_if(\mathbf{x_i})-1+\xi_i)=0$$
            - 4.$$\xi_i\geq0,\mu_i\xi_i=0$$ 

        > 当$$\lambda_i>0$$,则必有$$y_if(\mathbf{x}_i)=1-\xi_i$$,即该样本是支持向量
        >
        > 当$$\lambda_i<C$$,则$$\mu_i>0$$,进而有$$\xi_i=0$$,即该样本恰在最大间隔边界上
        > 
        > 当$$\lambda_i=C$$,则有$$\mu_i=0$$,此时若$$\xi_i\leq1$$则该样本落在最大间隔内部，若$$\xi_i>1$$则该样本被错误分类
        > 


### 参考
- 《机器学习》周志华
