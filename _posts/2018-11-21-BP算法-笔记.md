---
layout:     post   				                            # 使用的布局（不需要改）
title:      神经网络				                   # 标题 
subtitle:   BP算法            # 副标题
date:       2018-11-21 				                        # 时间
author:     RS-Wang 						                      # 作者
header-img: img/post-bg-ml2.jpg 	                    #这篇文章标题背景图片
catalog: true 						                            # 是否归档
usemathjax: true
tags:								                                  #标签
tags:                                                                 #标签
    - 机器学习
    - 神经网络
    - BP算法
    - 监督学习
---

### 1.神经网络
- #### 1.1.神经元 
  ![](https://note.youdao.com/yws/api/personal/file/C620BA8E1A1740568536452B3DE6144F?method=download&shareKey=c407810792591dfec3ed267fb919e7a8)

    - 神经元接收其他n个神经元的来的信号，这些输入神经元信号通过权重连接传递，神经元收到总输入值域阈值比较，然后通过激活函数处理产生神经元的输出。

- #### 1.2.激活函数
  ![](https://note.youdao.com/yws/api/personal/file/09274A75622F42C788FA437FBC0C1AA0?method=download&shareKey=3055c4d0329105a20dc4bdb1934cb976)
    
    - 阶跃函数
        - 它将输入值映射成0或1，0对应神经元抑制，1对应神经元兴奋
        - 理想中的激活函数，但函数具有不连续、不光滑等不好的性质
        - 实际中常用Sigmoid函数做激活函数
    
    - Sigmoid函数
        - 它将变化范围大的输入值映射成(0,1)范围内，进行输出
        - 具有连续、光滑、可导等性质

- #### 1.3.多层前馈神经网络
  ![](https://note.youdao.com/yws/api/personal/file/03A14724C0AF461DB4E2C0592437B1D6?method=download&shareKey=345aa6e948fe94d69b82c52bbc5a4cfa) 
    
    - 定义：每层神经元与下一层神经元全互连，同层不相互连接，也不进行跨层连接
    - 多层神经网络可以解决非线性可分问题
    - 多层前馈神经网络只要包含足够多的隐层，就能以任意精度逼近任意复杂度的连续函数

### 2.BP算法
- #### 2.1.介绍
    - 作用：多层神经网络的学习能力要比单层感知机强得多，那么就需要更强的学习算法。BP(误差逆传播)算法就是其中最杰出代表，它不仅可用于多层前馈神经网络，还可以适用于其他类型网络如递归神经网络。
    - 核心：使用梯度下降和链式求导法则

- #### 2.2.激活函数
    - 2.2.1.激活函数
    $$Sigmoid(z)=\sigma(z)=\frac{1}{1+e^{-z}}$$
    
    - 2.2.2.激活函数求导
        - Sigmoid(z)对z求导：
        $$
        \begin{equation}
        \begin{aligned}
        \frac{\partial\sigma(z)}{\partial z} &= \frac{e^{-z}}{(1+e^{-z})^2}\\
        &=\frac{1+e^{-z}-1}{(1+e^{-z})^2}\\
        &=\frac{1}{1+e^{-z}}-\frac{1}{(1+e^{-z})^2}\\
        &=\sigma(z)(1-\sigma(z))
        \end{aligned}
        \end{equation}
        $$

- #### 2.3.BP算法推导
    - 2.3.1.简单网络拓扑结构示例
    
    ![](https://note.youdao.com/yws/api/personal/file/A4734F9819EC4327B4E6E8983388AA43?method=download&shareKey=54e593c1d525727d1147dd1afa6ecd2f)
    - 2.3.2.符号定义
        - $$x^l_j$$：层L下的节点j的输入
        - $$w^l_{i,j}$$：从层L-1中的节点i到层L中的节点j这一段的权重
        - $$\theta^l_j$$：层L下的节点j的偏置
        - $$O^l_j$$：层L下的节点j的输出
        - $$y_j$$：目标值
        - $$E=\frac{1}{2}\sum_{k\in K}(O_k-y_k)^2$$：均方误差
    - 2.3.3.计算x和O值示例
        - 1.计算$$x_3$$:$$x_3=W_{1,3}x_1+W_{2,3}x_2$$
        - 2.计算$$O_3$$:$$O_3=\sigma(x_3)$$
        
    - 2.3.4.更新权值示例(为了简便没带偏置项)
        - 1.图中各个x和O的值：
	
        $$
        \begin{equation}
        \begin{aligned}
        E&=\frac{1}{2}(O_5-y_5)^2\\
        O_5&=\sigma(x_5)\\
        x_5&=W_{3,5}\cdot O_3+W_{4,5}\cdot O_4\\
        O_3&=\sigma(x_3)\\
        x_3&=W_{1,3}x_1+W_{2,3}x_2
        \end{aligned}
        \end{equation}
        $$

        - 2.求出$$g(W_{1,3})$$的值(链式法则)：
	
        $$
        \begin{equation}
        \begin{aligned}
        g(W_{1,3})&=\frac{\partial E}{\partial W_{1,3}}\\
        &=\frac{\partial E}{\partial O_5}\cdot\frac{\partial O_5}{\partial x_5}\cdot\frac{\partial x_5}{\partial O_3}\cdot\frac{\partial O_3}{\partial x_3}\cdot\frac{\partial x_3}{\partial W_{1,3}}\\
        &=(O_5-y_5)\cdot\underbrace{\sigma(x_5)}_{O_5}\cdot\underbrace{(1-\sigma(x_5))}_{1-O_5}\cdot W_{3,5}\cdot\underbrace{\sigma(x_3)}_{O_3}\cdot\underbrace{(1-\sigma(x_3))}_{1-O_3}\cdot x_1
        \end{aligned}
        \end{equation}
        $$
            
	    - 在上式中可以看出，g(W)的值是由现有的值直接计算得出，不需要求解过程，速度非常快
			
        - 3.更新$$W_{1,3}$$的值(使用梯度下降法):
	
        $$
        \begin{equation}
        \begin{aligned}
        W_{1,3}^{new} = W_{1,3}^{old} - \alpha g(W_{1,3}^{old})
        \end{aligned}
        \end{equation}
        $$

    - 2.3.5.各层的梯度值：
        - 1.输出层(连接权W)
            - 计算目标:$$g(W_{j,k})=\frac{\partial E}{\partial W_{j,k}}$$  
            - 具体计算：
	    
            $$
            \begin{equation}
            \begin{aligned}
            g(W_{j,k})&=\frac{\partial}{\partial O_k}\frac{1}{2}\sum_{k\in K}(O_k-y_k)^2\cdot\frac{\partial O_k}{\partial x_k}\cdot\frac{\partial x_k}{\partial W_{j,k}}\\
            &=(O_k-y_k)\cdot\sigma(x_k)\cdot(1-\sigma(x_k))\cdot O_j\\
            &=\underbrace{(O_k-y_k)\cdot O_k\cdot(1-O_k)}_{\delta_k}\cdot O_j
            \end{aligned}
            \end{equation}
            $$

        - 2.输出层(偏置项$$\theta$$)
            - 计算目标:$$g(\theta_k)=\frac{\partial E}{\partial\theta_k}$$
            - 具体计算：
	    
            $$
            \begin{equation}
            \begin{aligned}
            g(\theta_k)&=\frac{\partial}{\partial O_k}\frac{1}{2}\sum_{k\in K}(O_k-y_k)^2\cdot\frac{\partial O_k}{\partial x_k}\cdot\frac{\partial x_k}{\partial\theta_k}\\
            &=(O_k-y_k)\cdot\sigma(x_k)\cdot(1-\sigma(x_k))\\
            &=\underbrace{(O_k-y_k)\cdot O_k\cdot(1-O_k)}_{\delta_k}
            \end{aligned}
            \end{equation}
            $$

        - 3.隐藏层(连接权W)
            - 计算目标：$$g(W_{i,j})=\frac{\partial E}{\partial W_{i,j}}$$ 
            - 具体计算：
	    
            $$
            \begin{equation}
            \begin{aligned}
            g(W_{i,j})&=\underbrace{\sum_{k\in K}(O_k-y_k)\cdot\sigma(x_k)\cdot(1-\sigma(x_k))\cdot W_{j,k}}_{\sum_{k\in K}\delta_kW_{j,k}}\cdot O_j\cdot(1-O_j)\cdot O_i\\
            &=\underbrace{\sum_{k\in K}\delta_k\cdot W_{j,k}\cdot O_j\cdot(1-O_j)}_{\delta_j}\cdot O_i\\
            &=\delta_j\cdot O_i
            \end{aligned}
            \end{equation}
            $$

        - 3.隐藏层(偏置项$$\theta$$)
            - 计算目标：$$g(\theta_j)=\frac{\partial E}{\partial\theta_j}$$ 
            - 具体计算：
	    
            $$
            \begin{equation}
            \begin{aligned}
            g(\theta_j)&=\underbrace{\sum_{k\in K}(O_k-y_k)\cdot\sigma(x_k)\cdot(1-\sigma(x_k))\cdot W_{j,k}}_{\sum_{k\in K}\delta_kW_{j,k}}\cdot O_j\cdot(1-O_j)\\
            &=\underbrace{\sum_{k\in K}\delta_k\cdot W_{j,k}\cdot O_j\cdot(1-O_j)}_{\delta_j}
            \end{aligned}
            \end{equation}
            $$    


    - 2.3.6.总结
        - 1.输出层节点$$k\in K$$:
            - 1.1.连接权W：
	    
            $$
            \begin{equation}
            \begin{aligned}
            g(W_{j,k})&=\frac{\partial E}{\partial W_{j,k}}\\
            &=\delta_k\cdot O_j
            \end{aligned}
            \end{equation}
            $$ 

            - 1.2.偏置项$$\theta$$:
	    
            $$
            \begin{equation}
            \begin{aligned}
            g(\theta_k)&=\frac{\partial E}{\partial\theta_k}\\
            &=\delta_k
            \end{aligned}
            \end{equation}
            $$

            - 其中:$$\delta_k=(O_k-y_k)\cdot O_k\cdot(1-O_k)$$
            
        - 2.隐层节点$$j\in J$$：
            - 2.1.连接权W：
	    
            $$
            \begin{equation}
            \begin{aligned}
            g(W_{i,j})&=\frac{\partial E}{\partial W_{i,j}}\\
            &=\delta_j\cdot O_i
            \end{aligned}
            \end{equation}
            $$

            - 2.2.偏置项$$\theta$$:
	    
            $$
            \begin{equation}
            \begin{aligned}
            g(\theta_j)&=\frac{\partial E}{\partial\theta_j}\\
            &=\delta_j
            \end{aligned}
            \end{equation}
            $$

            - 其中:$$\delta_j=\sum_{k\in K}\delta_k\cdot W_{j,k}\cdot O_j\cdot(1-O_j)$$

- #### 2.4.BP算法步骤
    - 输入：数据集D
    - 过程：
        - 在(0,1)范围内随机初始化所有的里连接权和阈值
        - repeat
            - for all $$(x_k,y_k)\in D$$ do
                - 根据当前参数计算出$$O_k$$
                - 计算输出层的$$\delta_k$$
                - 计算隐藏层的$$\delta_j$$
                - 根据 9-12 式更新出连接权$$W_{j,k},W_{i,j}$$与阈值$$\theta_k,\theta_j$$
            - end for
        - until 达到停止条件
    - 输出：连接权和阈值确定的多层前馈神经网络

### 参考
- 《机器学习》周志华
- 《优化论五部曲》Jason博士
