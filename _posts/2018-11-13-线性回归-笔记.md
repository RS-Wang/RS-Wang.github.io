---
layout:     post   				                            # 使用的布局（不需要改）
title:      线性回归				                   # 标题 
subtitle:   线性回归            # 副标题
date:       2018-11-13 				                        # 时间
author:     RS-Wang 						                      # 作者
header-img: img/post-bg-cook.jpg 	                    #这篇文章标题背景图片
catalog: true 						                            # 是否归档
usemathjax: true
tags:								                                  #标签
    - 西瓜书
    - 机器学习
    - 线性回归
---

## 线性回归

### 1线性映射关系: $$ y = XW + b $$
- X 为(m,n)矩阵，W 为(n,1)矩阵
- 将X多加一列值为1，W(W,b)，公式可以简化为: $$ y = XW $$

### 2损失函数（极小化）：
- 一般表示：
    $$ J(W)= \frac{1}{m}\sum_{i=1}^{m}(\hat y_i - y_i)^2 $$
- 向量化表示：
    $$ J(W)= \frac{1}{m}||y-XW||^2_2 $$

### 3损失函数的由来:
- #### 3.1最小二乘法思想：
> 最合理的参数估计量应该使得模型能最好地拟合样本数据，也就是估计值和观测
值之差的平方和最小，其推导过程如下所示
>
>$$ J(W)= \sum_{i=1}^{m}(\hat y_i - y_i)^2 $$
- #### 3.2极大似然估计思想：
>最合理的参数估计量应该使得从模型中抽取 m 组样本观测值的概率极大，也就是
似然函数极大
>
>1.假设 $$ b\sim N(0,\sigma^2) $$,那么$$ y_i\sim N(x_iW,\sigma^2) $$,就可以得出下面的似然函数：

    >$$
    \begin{equation}
    \begin{aligned}
    L(W) &=\Pi^m_{i=1}P(y_i|x_i;W)\\
    &=\Pi^m_{i=1}\frac{1}{\sqrt{2\pi}\sigma}\exp-\frac{(y_i-x_iW)^2}{2\sigma^2}
    \end{aligned}
    \end{equation}
    $$
>
>2.对似然函数进行取对数,令 $$ H(W) = \ln L(W) $$:

    >$$
    \begin{equation}
    \begin{aligned}
    H(W) &=\ln L(W)\\
    &=\ln (\Pi^m_{i=1}\frac{1}{\sqrt{2\pi}\sigma}\exp-\frac{(y_i-x_iW)^2}{2\sigma^2})\\
    &=\sum^m_{i=1}\ln(\frac{1}{\sqrt{2\pi}\sigma}\exp-\frac{(y_i-x_iW)^2}{2\sigma^2})\\
    &=-m\ln\sqrt{2\pi}\sigma -\frac{1}{2\sigma^2}\sum^m_{i=1}(y_i-x_iW)^2\\
    \end{aligned}
    \end{equation}
     $$
>
>3.求 $$ H(W) $$ 最大值，即求 $$ \frac{\partial{H(W)}}{\partial{W}}=0 $$,可以看出求导只与 $$ \sum^m_{i=1}(y_i-x_iW)^2 $$ 有关。
>
>4.令 $$J(W)= \frac{1}{m} \parallel y-XW \parallel^2_2$$
>
>5.可得出 $$ \mathop{\arg\max}_{W} H(w) \Leftrightarrow \mathop{\arg\min}_{W}J(W) $$

### 4.求解 $$ \mathop{\arg\min}_{W} J(W) $$ : 
- 4.1因为求 $$ \mathop{\arg\min}_{W}\frac{1}{m} \parallel y-XW \parallel^2_2 \Leftrightarrow \mathop{\arg\min}_{W} \frac{1}{2m}\parallel y-XW \parallel^2_2$$
- 4.2那么将 $$ J(W) = \frac{1}{2m}||y-XW||^2_2 $$展开可得下面式子:
    >
    >$$
    \begin{equation}
    \begin{aligned}
    J(W) & = \frac{1}{2m}||y-XW||^2_2\\
    & = \frac{1}{2m}(y-XW)^T(y-XW)\\
    & = \frac{1}{2m}(y^Ty-y^TXW-W^TX^Ty+W^TX^TXW)\\
    & = \frac{1}{2m}(y^Ty-2y^TXW+W^TX^TXW)
    \end{aligned}
    \end{equation}
    $$
    >
	
- 4.3求解有两种方法：
    - 4.3.1.直接求:$$ \frac{\partial{J(W)}}{\partial{W}}=0 $$ 
    >$$
    \begin{equation}
    \begin{aligned}
    \frac{\partial{J(W)}}{\partial{W}} = J(W)^{'} &= 0\\
    \Rightarrow X^TXW - X^Ty & = 0\\
    W^* & = (X^TX)^{-1}X^Ty
    \end{aligned}
    \end{equation}
    $$ 
	>
        - 这里有个缺陷就是求$$ X^TX $$的逆
    - 4.3.2无约束优化迭代法：
        - 梯度下降法
            - 1.令$$ g(W)=J(W)^{'} $$
            >$$
            \begin{equation}
            \begin{aligned}
            g(W) & = J(W)^{'}\\
            & = X^TXW - X^Ty\\
            & = X^T(XW - y)
            \end{aligned}
            \end{equation}
            $$
            - 2.然后重复迭代：$$ W^{new} = W^{old} - \alpha g(W^{old}) $$ 直到评价函数小于你设定的容忍度为止
            - 3.这里只使用了泰勒级数展开一阶来近似会比牛顿法和拟牛顿法要慢
            - 4.随机梯度下降和随机批量梯度下降在现实中更常用
    - 无约束优化迭代法详细请看
### 5参考
- 《机器学习》周志华

